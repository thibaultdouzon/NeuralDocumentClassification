{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clemsage/NeuralDocumentClassification/blob/master/skeleton_ocr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "56cA2WtUQZ-I"
      },
      "source": [
        "# Training a classifier on OCR text input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AAd75X5JT4vI"
      },
      "source": [
        "# Imports & Cloning repository\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mVIlDgNHW62j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from os import path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_names = [\"email\", \"form\", \"handwritten\", \"invoice\", \"advertisement\"]\n",
        "NUM_CLASSES = len(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists(\"NeuralDocumentClassification\"):\n",
        "    !git clone https://github.com/thibaultdouzon/NeuralDocumentClassification.git\n",
        "else:\n",
        "    !git -C NeuralDocumentClassification pull\n",
        "sys.path.append(\"NeuralDocumentClassification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src import download_dataset\n",
        "\n",
        "dataset_path = \"dataset\"\n",
        "\n",
        "download_dataset.download_and_extract(\"all\", dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(path.join(dataset_path, \"train.pkl\"), \"rb\") as f:\n",
        "    train_dataset = pickle.load(f)\n",
        "\n",
        "with open(path.join(dataset_path, \"test.pkl\"), \"rb\") as f:\n",
        "    test_dataset = pickle.load(f)\n",
        "\n",
        "with open(path.join(dataset_path, \"validation.pkl\"), \"rb\") as f:\n",
        "    validation_dataset = pickle.load(f)\n",
        "\n",
        "\n",
        "for split_name, split_dataset in zip(\n",
        "    [\"train\", \"test\", \"validation\"], [train_dataset, test_dataset, validation_dataset]\n",
        "):\n",
        "    print(f\"{split_name}_dataset contains {len(split_dataset)} documents\")\n",
        "train_dataset[0].keys()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each `dataset` object is a `list` containing multiple document information. A document is a `dict` with the following structure:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"id\": \"Unique document identifier\",\n",
        "  \"image\": \"A PIL.Image object containing the document's image\",\n",
        "  \"label\": \"A number between in [0 .. 4] representing the class of the document\",\n",
        "  \"words\": \"A list of strings (not words !) extracted from the image with an OCR\",\n",
        "  \"boxes\": \"A list of tuples of numbers providing the position of each word in the document\"\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uefjUmcMRnLT"
      },
      "source": [
        "# Explore the data\n",
        "\n",
        "Take the time to explore the textual data included in the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ideas\n",
        "\n",
        "- 10 most common words? (hint: Counter)\n",
        "- Count number of unique words\n",
        "- Distribution of words (cumulative occurences plot)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "all_texts = [\n",
        "    [word for sentence in doc[\"words\"] for word in sentence.split()]\n",
        "    for doc in validation_dataset + test_dataset + train_dataset\n",
        "]\n",
        "\n",
        "most_common_words = Counter([w for text in all_texts for w in text])\n",
        "most_common_words.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "n_unique_words = len({w for text in all_texts for w in text})\n",
        "n_unique_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "# Zipf's law\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(\n",
        "    [c / sum(most_common_words.values()) for w, c in most_common_words.most_common(50)]\n",
        ")\n",
        "\n",
        "# put words on xlabel\n",
        "plt.xticks(\n",
        "    range(50),\n",
        "    [w for w, c in most_common_words.most_common(50)],\n",
        "    rotation=80,\n",
        "    fontsize=9,\n",
        ")\n",
        "plt.ylabel(\"Word frequency\")\n",
        "plt.title(\"Word frequency in the dataset\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "from itertools import accumulate\n",
        "\n",
        "cum_word_occurences = list(\n",
        "    accumulate([count for word, count in most_common_words.most_common(n_unique_words)])\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(cum_word_occurences)\n",
        "\n",
        "plt.xlabel(\"Rank of the word\")\n",
        "plt.ylabel(\"Number of occurences\")\n",
        "plt.title(\"Cumulative number of occurences of the most common words\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KNr1m0q_Tjo9"
      },
      "source": [
        "# Classification with Scikit Learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WheOgS6TW62r"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import sklearn\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TextSample:\n",
        "    text: str\n",
        "    label: int\n",
        "\n",
        "    def __init__(self, document: dict):\n",
        "        self.text = \" \".join(\n",
        "            [word for sentence in document[\"words\"] for word in sentence.split()]\n",
        "        )\n",
        "        self.label = document[\"label\"]\n",
        "\n",
        "\n",
        "train_samples = [TextSample(doc) for doc in train_dataset]\n",
        "\n",
        "test_samples = [TextSample(doc) for doc in test_dataset]\n",
        "\n",
        "validation_samples = [TextSample(doc) for doc in validation_dataset]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization and Vectorization\n",
        "\n",
        "To train models at solving our problem, we need to convert texts into vectors that will represent our documents.\n",
        "Take a look at Scikit Learn [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#) and [TFIDFVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
        "First fit a vectorizer on the training set, then apply the vectorization transformation to each dataset split.\n",
        "\n",
        "What are the shapes of the resulting vectors? What does each dimension mean?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ngR65gEjW62w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform([sample.text for sample in train_samples])\n",
        "X_test = vectorizer.transform([sample.text for sample in test_samples])\n",
        "X_validation = vectorizer.transform([sample.text for sample in validation_samples])\n",
        "\n",
        "Y_train = [sample.label for sample in train_samples]\n",
        "Y_test = [sample.label for sample in test_samples]\n",
        "Y_validation = [sample.label for sample in validation_samples]\n",
        "\n",
        "\n",
        "X_train.shape, X_test.shape, X_validation.shape\n",
        "# Each vector's first dimension is the number of documents, the second dimension is the number of unique words in the dataset\n",
        "# The value at (i, j) is the number of occurences of the j-th word in the i-th document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FFHy4rtVW63D"
      },
      "source": [
        "## Basic Model: Scikit-Learn Classification\n",
        "\n",
        "Use any Scikit-Learn classification model to train a first text model.\n",
        "Good first picks: [Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) or [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(kernel=\"linear\")\n",
        "model.fit(X_train, Y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the model\n",
        "\n",
        "Use Scikit-Learn [metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) to evaluate your model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "print(\"Test\")\n",
        "Y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(f\"Accuracy on the test set: {accuracy:.2f}\")\n",
        "print(confusion_matrix(Y_test, Y_pred))\n",
        "\n",
        "print(\"Validation\")\n",
        "Y_pred = model.predict(X_validation)\n",
        "accuracy = accuracy_score(Y_validation, Y_pred)\n",
        "print(f\"Accuracy on the validation set: {accuracy:.2f}\")\n",
        "print(confusion_matrix(Y_validation, Y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QoTPVaKYW63Q"
      },
      "source": [
        "# Transformers\n",
        "\n",
        "Done playing with kids toys.\n",
        "\n",
        "All modern AI models use the [Transformer architecture](https://arxiv.org/pdf/1706.03762). The initial research paper is one of the most influencial of the last decade.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from torch import nn\n",
        "from torch.utils import data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization\n",
        "\n",
        "Transformers usually use subword tokenizer, ie. a word _can_ be tokenized into multiple tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YN3QZJaAbN6-"
      },
      "outputs": [],
      "source": [
        "# Let's use LayoutLM tokenizer first\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    \"microsoft/layoutlm-base-uncased\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoding = tokenizer(\"Hello, world! I can tokenize any sentence.\")\n",
        "\n",
        "for token_id in encoding[\"input_ids\"]:\n",
        "    print(tokenizer.decode(token_id))\n",
        "\n",
        "# Note how `tokenize` is encoded as `token ##ize`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset for LayoutLM\n",
        "\n",
        "LayoutLM uses both textual and 2D positional information, here is a new data sample class to work with\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_sentence_into_words(\n",
        "    sentence: str, sentence_box: list[int, int, int, int]\n",
        ") -> tuple[list[str], list[tuple[int, int, int, int]]]:\n",
        "    ret_words = []\n",
        "    ret_boxes = []\n",
        "    words = sentence.split()\n",
        "\n",
        "    ret_words.extend(words)\n",
        "\n",
        "    words_len = [len(word) for word in words]\n",
        "    box_width = sentence_box[2] - sentence_box[0]\n",
        "\n",
        "    word_left = sentence_box[0]\n",
        "    for word_len in words_len:\n",
        "        word_right = word_left + int(word_len * box_width / len(sentence))\n",
        "        ret_boxes.append((word_left, sentence_box[1], word_right, sentence_box[3]))\n",
        "        word_left = word_right + int(1 * box_width / len(sentence))\n",
        "\n",
        "    return ret_words, ret_boxes\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TextBoxSample:\n",
        "    words: list[str]\n",
        "    boxes: list[tuple[int, int, int, int]]  # (left, top, right, bottom)\n",
        "    label: int\n",
        "\n",
        "    def __init__(self, document: dict):\n",
        "        self.words = []\n",
        "        self.boxes = []\n",
        "\n",
        "        # We need to split the words in the sentences and compute the bounding boxes for each word\n",
        "        for sentence, sentence_box in zip(document[\"words\"], document[\"boxes\"]):\n",
        "            new_words, new_boxes = split_sentence_into_words(sentence, sentence_box)\n",
        "            self.words.extend(new_words)\n",
        "            self.boxes.extend(new_boxes)\n",
        "\n",
        "        self.label = document[\"label\"]\n",
        "\n",
        "\n",
        "train_samples = [TextBoxSample(doc) for doc in train_dataset]\n",
        "\n",
        "test_samples = [TextBoxSample(doc) for doc in test_dataset]\n",
        "\n",
        "validation_samples = [TextBoxSample(doc) for doc in validation_dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's implement the pytorch dataset that will hold those samples. Keep it very simple, we will delay most computation to the batching function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextBoxDataset(data.Dataset):\n",
        "    def __init__(self, samples: list[TextBoxSample]):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __getitem__(self, idx: int) -> TextBoxSample:\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @tilte\n",
        "\n",
        "\n",
        "class TextBoxDataset(data.Dataset):\n",
        "    def __init__(self, samples: list[TextBoxSample]):\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx) -> TextBoxSample:\n",
        "        return self.samples[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batching function\n",
        "\n",
        "Like we did in the vision part, we need to implement a batching function that will batch together multiple inputs together and prepare them to be fed to the model.\n",
        "\n",
        "Huggingface transformers tokenizers have the hability to tokenize a whole batch at once and perform most of the computation for us.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /Users/thibaultdouzon/.cache/huggingface/hub/models--microsoft--layoutlmv2-base-uncased/snapshots/ae6f4350c668f88ec580046e35c670df6ec616c1/config.json\n",
            "Model config LayoutLMv2Config {\n",
            "  \"_name_or_path\": \"microsoft/layoutlmv2-base-uncased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"convert_sync_batchnorm\": true,\n",
            "  \"coordinate_size\": 128,\n",
            "  \"detectron2_config_args\": {\n",
            "    \"MODEL.ANCHOR_GENERATOR.SIZES\": [\n",
            "      [\n",
            "        32\n",
            "      ],\n",
            "      [\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        128\n",
            "      ],\n",
            "      [\n",
            "        256\n",
            "      ],\n",
            "      [\n",
            "        512\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.BACKBONE.NAME\": \"build_resnet_fpn_backbone\",\n",
            "    \"MODEL.FPN.IN_FEATURES\": [\n",
            "      \"res2\",\n",
            "      \"res3\",\n",
            "      \"res4\",\n",
            "      \"res5\"\n",
            "    ],\n",
            "    \"MODEL.MASK_ON\": true,\n",
            "    \"MODEL.PIXEL_STD\": [\n",
            "      57.375,\n",
            "      57.12,\n",
            "      58.395\n",
            "    ],\n",
            "    \"MODEL.POST_NMS_TOPK_TEST\": 1000,\n",
            "    \"MODEL.RESNETS.ASPECT_RATIOS\": [\n",
            "      [\n",
            "        0.5,\n",
            "        1.0,\n",
            "        2.0\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.RESNETS.DEPTH\": 101,\n",
            "    \"MODEL.RESNETS.NUM_GROUPS\": 32,\n",
            "    \"MODEL.RESNETS.OUT_FEATURES\": [\n",
            "      \"res2\",\n",
            "      \"res3\",\n",
            "      \"res4\",\n",
            "      \"res5\"\n",
            "    ],\n",
            "    \"MODEL.RESNETS.SIZES\": [\n",
            "      [\n",
            "        32\n",
            "      ],\n",
            "      [\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        128\n",
            "      ],\n",
            "      [\n",
            "        256\n",
            "      ],\n",
            "      [\n",
            "        512\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.RESNETS.STRIDE_IN_1X1\": false,\n",
            "    \"MODEL.RESNETS.WIDTH_PER_GROUP\": 8,\n",
            "    \"MODEL.ROI_BOX_HEAD.NAME\": \"FastRCNNConvFCHead\",\n",
            "    \"MODEL.ROI_BOX_HEAD.NUM_FC\": 2,\n",
            "    \"MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\": 14,\n",
            "    \"MODEL.ROI_HEADS.IN_FEATURES\": [\n",
            "      \"p2\",\n",
            "      \"p3\",\n",
            "      \"p4\",\n",
            "      \"p5\"\n",
            "    ],\n",
            "    \"MODEL.ROI_HEADS.NAME\": \"StandardROIHeads\",\n",
            "    \"MODEL.ROI_HEADS.NUM_CLASSES\": 5,\n",
            "    \"MODEL.ROI_MASK_HEAD.NAME\": \"MaskRCNNConvUpsampleHead\",\n",
            "    \"MODEL.ROI_MASK_HEAD.NUM_CONV\": 4,\n",
            "    \"MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\": 7,\n",
            "    \"MODEL.RPN.IN_FEATURES\": [\n",
            "      \"p2\",\n",
            "      \"p3\",\n",
            "      \"p4\",\n",
            "      \"p5\",\n",
            "      \"p6\"\n",
            "    ],\n",
            "    \"MODEL.RPN.POST_NMS_TOPK_TRAIN\": 1000,\n",
            "    \"MODEL.RPN.PRE_NMS_TOPK_TEST\": 1000,\n",
            "    \"MODEL.RPN.PRE_NMS_TOPK_TRAIN\": 2000\n",
            "  },\n",
            "  \"fast_qkv\": true,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"has_relative_attention_bias\": true,\n",
            "  \"has_spatial_attention_bias\": true,\n",
            "  \"has_visual_segment_embedding\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"image_feature_pool_shape\": [\n",
            "    7,\n",
            "    7,\n",
            "    256\n",
            "  ],\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_2d_position_embeddings\": 1024,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_rel_2d_pos\": 256,\n",
            "  \"max_rel_pos\": 128,\n",
            "  \"model_type\": \"layoutlmv2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rel_2d_pos_bins\": 64,\n",
            "  \"rel_pos_bins\": 32,\n",
            "  \"shape_size\": 128,\n",
            "  \"transformers_version\": \"4.45.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /Users/thibaultdouzon/.cache/huggingface/hub/models--microsoft--layoutlmv2-base-uncased/snapshots/ae6f4350c668f88ec580046e35c670df6ec616c1/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /Users/thibaultdouzon/.cache/huggingface/hub/models--microsoft--layoutlmv2-base-uncased/snapshots/ae6f4350c668f88ec580046e35c670df6ec616c1/config.json\n",
            "Model config LayoutLMv2Config {\n",
            "  \"_name_or_path\": \"microsoft/layoutlmv2-base-uncased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"convert_sync_batchnorm\": true,\n",
            "  \"coordinate_size\": 128,\n",
            "  \"detectron2_config_args\": {\n",
            "    \"MODEL.ANCHOR_GENERATOR.SIZES\": [\n",
            "      [\n",
            "        32\n",
            "      ],\n",
            "      [\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        128\n",
            "      ],\n",
            "      [\n",
            "        256\n",
            "      ],\n",
            "      [\n",
            "        512\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.BACKBONE.NAME\": \"build_resnet_fpn_backbone\",\n",
            "    \"MODEL.FPN.IN_FEATURES\": [\n",
            "      \"res2\",\n",
            "      \"res3\",\n",
            "      \"res4\",\n",
            "      \"res5\"\n",
            "    ],\n",
            "    \"MODEL.MASK_ON\": true,\n",
            "    \"MODEL.PIXEL_STD\": [\n",
            "      57.375,\n",
            "      57.12,\n",
            "      58.395\n",
            "    ],\n",
            "    \"MODEL.POST_NMS_TOPK_TEST\": 1000,\n",
            "    \"MODEL.RESNETS.ASPECT_RATIOS\": [\n",
            "      [\n",
            "        0.5,\n",
            "        1.0,\n",
            "        2.0\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.RESNETS.DEPTH\": 101,\n",
            "    \"MODEL.RESNETS.NUM_GROUPS\": 32,\n",
            "    \"MODEL.RESNETS.OUT_FEATURES\": [\n",
            "      \"res2\",\n",
            "      \"res3\",\n",
            "      \"res4\",\n",
            "      \"res5\"\n",
            "    ],\n",
            "    \"MODEL.RESNETS.SIZES\": [\n",
            "      [\n",
            "        32\n",
            "      ],\n",
            "      [\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        128\n",
            "      ],\n",
            "      [\n",
            "        256\n",
            "      ],\n",
            "      [\n",
            "        512\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.RESNETS.STRIDE_IN_1X1\": false,\n",
            "    \"MODEL.RESNETS.WIDTH_PER_GROUP\": 8,\n",
            "    \"MODEL.ROI_BOX_HEAD.NAME\": \"FastRCNNConvFCHead\",\n",
            "    \"MODEL.ROI_BOX_HEAD.NUM_FC\": 2,\n",
            "    \"MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\": 14,\n",
            "    \"MODEL.ROI_HEADS.IN_FEATURES\": [\n",
            "      \"p2\",\n",
            "      \"p3\",\n",
            "      \"p4\",\n",
            "      \"p5\"\n",
            "    ],\n",
            "    \"MODEL.ROI_HEADS.NAME\": \"StandardROIHeads\",\n",
            "    \"MODEL.ROI_HEADS.NUM_CLASSES\": 5,\n",
            "    \"MODEL.ROI_MASK_HEAD.NAME\": \"MaskRCNNConvUpsampleHead\",\n",
            "    \"MODEL.ROI_MASK_HEAD.NUM_CONV\": 4,\n",
            "    \"MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\": 7,\n",
            "    \"MODEL.RPN.IN_FEATURES\": [\n",
            "      \"p2\",\n",
            "      \"p3\",\n",
            "      \"p4\",\n",
            "      \"p5\",\n",
            "      \"p6\"\n",
            "    ],\n",
            "    \"MODEL.RPN.POST_NMS_TOPK_TRAIN\": 1000,\n",
            "    \"MODEL.RPN.PRE_NMS_TOPK_TEST\": 1000,\n",
            "    \"MODEL.RPN.PRE_NMS_TOPK_TRAIN\": 2000\n",
            "  },\n",
            "  \"fast_qkv\": true,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"has_relative_attention_bias\": true,\n",
            "  \"has_spatial_attention_bias\": true,\n",
            "  \"has_visual_segment_embedding\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"image_feature_pool_shape\": [\n",
            "    7,\n",
            "    7,\n",
            "    256\n",
            "  ],\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_2d_position_embeddings\": 1024,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_rel_2d_pos\": 256,\n",
            "  \"max_rel_pos\": 128,\n",
            "  \"model_type\": \"layoutlmv2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rel_2d_pos_bins\": 64,\n",
            "  \"rel_pos_bins\": 32,\n",
            "  \"shape_size\": 128,\n",
            "  \"transformers_version\": \"4.45.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "/Users/thibaultdouzon/Code/NeuralDocumentClassification/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /Users/thibaultdouzon/.cache/huggingface/hub/models--microsoft--layoutlmv2-base-uncased/snapshots/ae6f4350c668f88ec580046e35c670df6ec616c1/config.json\n",
            "Model config LayoutLMv2Config {\n",
            "  \"_name_or_path\": \"microsoft/layoutlmv2-base-uncased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"convert_sync_batchnorm\": true,\n",
            "  \"coordinate_size\": 128,\n",
            "  \"detectron2_config_args\": {\n",
            "    \"MODEL.ANCHOR_GENERATOR.SIZES\": [\n",
            "      [\n",
            "        32\n",
            "      ],\n",
            "      [\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        128\n",
            "      ],\n",
            "      [\n",
            "        256\n",
            "      ],\n",
            "      [\n",
            "        512\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.BACKBONE.NAME\": \"build_resnet_fpn_backbone\",\n",
            "    \"MODEL.FPN.IN_FEATURES\": [\n",
            "      \"res2\",\n",
            "      \"res3\",\n",
            "      \"res4\",\n",
            "      \"res5\"\n",
            "    ],\n",
            "    \"MODEL.MASK_ON\": true,\n",
            "    \"MODEL.PIXEL_STD\": [\n",
            "      57.375,\n",
            "      57.12,\n",
            "      58.395\n",
            "    ],\n",
            "    \"MODEL.POST_NMS_TOPK_TEST\": 1000,\n",
            "    \"MODEL.RESNETS.ASPECT_RATIOS\": [\n",
            "      [\n",
            "        0.5,\n",
            "        1.0,\n",
            "        2.0\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.RESNETS.DEPTH\": 101,\n",
            "    \"MODEL.RESNETS.NUM_GROUPS\": 32,\n",
            "    \"MODEL.RESNETS.OUT_FEATURES\": [\n",
            "      \"res2\",\n",
            "      \"res3\",\n",
            "      \"res4\",\n",
            "      \"res5\"\n",
            "    ],\n",
            "    \"MODEL.RESNETS.SIZES\": [\n",
            "      [\n",
            "        32\n",
            "      ],\n",
            "      [\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        128\n",
            "      ],\n",
            "      [\n",
            "        256\n",
            "      ],\n",
            "      [\n",
            "        512\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.RESNETS.STRIDE_IN_1X1\": false,\n",
            "    \"MODEL.RESNETS.WIDTH_PER_GROUP\": 8,\n",
            "    \"MODEL.ROI_BOX_HEAD.NAME\": \"FastRCNNConvFCHead\",\n",
            "    \"MODEL.ROI_BOX_HEAD.NUM_FC\": 2,\n",
            "    \"MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\": 14,\n",
            "    \"MODEL.ROI_HEADS.IN_FEATURES\": [\n",
            "      \"p2\",\n",
            "      \"p3\",\n",
            "      \"p4\",\n",
            "      \"p5\"\n",
            "    ],\n",
            "    \"MODEL.ROI_HEADS.NAME\": \"StandardROIHeads\",\n",
            "    \"MODEL.ROI_HEADS.NUM_CLASSES\": 5,\n",
            "    \"MODEL.ROI_MASK_HEAD.NAME\": \"MaskRCNNConvUpsampleHead\",\n",
            "    \"MODEL.ROI_MASK_HEAD.NUM_CONV\": 4,\n",
            "    \"MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\": 7,\n",
            "    \"MODEL.RPN.IN_FEATURES\": [\n",
            "      \"p2\",\n",
            "      \"p3\",\n",
            "      \"p4\",\n",
            "      \"p5\",\n",
            "      \"p6\"\n",
            "    ],\n",
            "    \"MODEL.RPN.POST_NMS_TOPK_TRAIN\": 1000,\n",
            "    \"MODEL.RPN.PRE_NMS_TOPK_TEST\": 1000,\n",
            "    \"MODEL.RPN.PRE_NMS_TOPK_TRAIN\": 2000\n",
            "  },\n",
            "  \"fast_qkv\": true,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"has_relative_attention_bias\": true,\n",
            "  \"has_spatial_attention_bias\": true,\n",
            "  \"has_visual_segment_embedding\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"image_feature_pool_shape\": [\n",
            "    7,\n",
            "    7,\n",
            "    256\n",
            "  ],\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_2d_position_embeddings\": 1024,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_rel_2d_pos\": 256,\n",
            "  \"max_rel_pos\": 128,\n",
            "  \"model_type\": \"layoutlmv2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rel_2d_pos_bins\": 64,\n",
            "  \"rel_pos_bins\": 32,\n",
            "  \"shape_size\": 128,\n",
            "  \"transformers_version\": \"4.45.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2013, 1024, 26429, 1010, 6338, 1039, 2006, 12256, 1025, 19802, 2756, 1010, 2722, 2184, 1024, 4466, 2572, 1015, 3395, 1024, 6819, 3527, 12928, 10374, 2013, 1039, 1012, 26429, 6764, 2000, 1024, 11404, 1025, 3021, 2332, 1024, 10507, 1024, 5003, 8747, 1025, 3744, 1025, 14264, 12170, 2100, 2436, 2001, 2333, 2197, 2733, 1998, 25610, 2368, 2128, 1066, 15242, 2070, 1997, 1996, 8378, 1997, 20868, 5358, 2665, 1000, 5371, 8208, 2433, 5585, 2549, 3074, 5491, 2234, 2408, 2628, 2011, 4809, 1997, 1996, 2392, 1997, 2261, 2678, 2696, 10374, 1025, 2228, 2122, 6876, 13262, 1041, 7959, 7610, 2278, 1025, 2031, 2042, 2513, 1998, 2020, 17153, 7652, 2098, 2000, 3765, 18808, 1999, 2254, 1997, 2023, 2095, 2043, 1031, 2187, 1011, 2021, 2245, 2009, 2190, 1056, 2692, 2191, 2469, 1012, 2097, 5860, 4232, 1996, 2065, 29337, 2064, 12210, 2008, 2122, 2678, 2696, 10374, 2024, 1999, 5527, 1010, 4809, 1997, 2037, 4486, 2027, 2019, 2063, 7610, 2072, 3314, 3539, 2099, 1006, 4098, 5349, 17550, 1007, 5170, 6384, 5305, 2311, 8715, 1006, 23848, 3630, 2614, 1004, 2678, 14412, 2544, 1007, 1996, 15536, 15265, 4115, 2177, 1006, 4098, 20464, 2140, 17550, 1007, 4913, 9482, 8129, 15775, 17471, 2078, 11366, 1006, 4098, 5349, 17550, 1007, 4098, 5349, 17550, 1007, 3891, 4806, 2852, 1024, 6294, 11053, 4283, 1025, 6338, 1024, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'bbox': [[0, 0, 0, 0], (145, 105, 176, 121), (145, 105, 176, 121), (182, 105, 231, 121), (182, 105, 231, 121), (237, 105, 261, 121), (267, 105, 273, 121), (279, 105, 291, 121), (297, 105, 321, 121), (297, 105, 321, 121), (327, 105, 345, 121), (351, 105, 369, 121), (351, 105, 369, 121), (375, 105, 399, 121), (405, 105, 436, 121), (405, 105, 436, 121), (405, 105, 436, 121), (442, 105, 454, 121), (697, 109, 717, 227), (145, 118, 190, 136), (145, 118, 190, 136), (195, 118, 251, 136), (195, 118, 251, 136), (195, 118, 251, 136), (195, 118, 251, 136), (256, 118, 278, 136), (283, 118, 294, 136), (283, 118, 294, 136), (299, 118, 338, 136), (343, 118, 371, 136), (145, 133, 163, 149), (145, 133, 163, 149), (169, 133, 205, 149), (169, 133, 205, 149), (211, 133, 235, 149), (247, 143, 279, 166), (247, 143, 279, 166), (145, 147, 164, 163), (145, 147, 164, 163), (170, 147, 208, 163), (170, 147, 208, 163), (170, 147, 208, 163), (214, 147, 245, 163), (214, 147, 245, 163), (281, 147, 325, 163), (144, 188, 162, 221), (144, 188, 162, 221), (168, 188, 204, 221), (210, 188, 228, 221), (234, 188, 264, 221), (270, 188, 294, 221), (300, 188, 324, 221), (330, 188, 348, 221), (354, 188, 378, 221), (354, 188, 378, 221), (384, 188, 396, 221), (395, 200, 427, 221), (395, 200, 427, 221), (429, 203, 453, 219), (459, 203, 471, 219), (477, 203, 495, 219), (501, 203, 531, 219), (537, 203, 549, 219), (451, 216, 479, 233), (451, 216, 479, 233), (295, 217, 326, 235), (332, 217, 363, 235), (332, 217, 363, 235), (369, 217, 413, 235), (419, 217, 444, 235), (507, 217, 525, 233), (507, 217, 525, 233), (531, 217, 592, 233), (145, 219, 209, 233), (217, 221, 242, 233), (248, 221, 286, 233), (145, 231, 189, 249), (194, 231, 205, 249), (210, 231, 243, 249), (248, 231, 259, 249), (264, 231, 280, 249), (285, 231, 312, 249), (317, 231, 328, 249), (343, 231, 360, 247), (365, 231, 429, 247), (365, 231, 429, 247), (365, 231, 429, 247), (365, 231, 429, 247), (445, 233, 473, 247), (478, 233, 506, 247), (511, 233, 545, 247), (367, 243, 393, 261), (304, 246, 328, 276), (304, 246, 328, 276), (336, 246, 369, 276), (336, 246, 369, 276), (336, 246, 369, 276), (145, 247, 169, 261), (175, 247, 199, 261), (205, 247, 253, 261), (259, 247, 277, 261), (283, 247, 307, 261), (401, 247, 454, 261), (401, 247, 454, 261), (401, 247, 454, 261), (459, 247, 470, 261), (475, 247, 522, 261), (475, 247, 522, 261), (527, 247, 538, 261), (144, 260, 180, 277), (185, 260, 195, 277), (200, 260, 221, 277), (226, 260, 247, 277), (252, 260, 273, 277), (278, 260, 283, 277), (288, 260, 309, 277), (314, 260, 319, 277), (369, 261, 391, 275), (397, 261, 435, 277), (440, 261, 450, 277), (455, 261, 476, 277), (481, 261, 491, 277), (481, 261, 491, 277), (496, 261, 517, 277), (522, 261, 549, 277), (522, 261, 549, 277), (459, 273, 481, 289), (486, 273, 525, 289), (486, 273, 525, 289), (530, 273, 546, 289), (147, 274, 174, 292), (147, 274, 174, 292), (179, 274, 195, 292), (200, 274, 238, 292), (243, 274, 265, 292), (270, 274, 297, 292), (302, 274, 357, 292), (302, 274, 357, 292), (302, 274, 357, 292), (362, 274, 378, 292), (383, 274, 394, 292), (399, 274, 443, 292), (399, 274, 443, 292), (145, 289, 175, 305), (180, 289, 190, 305), (195, 289, 220, 305), (225, 289, 255, 305), (263, 289, 289, 303), (295, 289, 314, 303), (295, 289, 314, 303), (145, 301, 164, 317), (145, 301, 164, 317), (170, 301, 208, 317), (214, 301, 252, 317), (214, 301, 252, 317), (258, 301, 302, 317), (258, 301, 302, 317), (258, 301, 302, 317), (308, 301, 333, 317), (308, 301, 333, 317), (145, 314, 182, 333), (188, 314, 225, 333), (231, 314, 255, 333), (261, 314, 310, 333), (316, 314, 365, 333), (371, 314, 408, 333), (371, 314, 408, 333), (371, 314, 408, 333), (414, 314, 445, 333), (451, 314, 457, 333), (463, 314, 494, 333), (505, 315, 525, 331), (531, 315, 584, 331), (531, 315, 584, 331), (251, 328, 269, 346), (275, 328, 325, 346), (275, 328, 325, 346), (275, 328, 325, 346), (331, 328, 362, 346), (368, 328, 411, 346), (368, 328, 411, 346), (368, 328, 411, 346), (368, 328, 411, 346), (417, 328, 442, 346), (417, 328, 442, 346), (145, 329, 171, 345), (177, 329, 243, 345), (145, 343, 177, 359), (183, 343, 228, 359), (183, 343, 228, 359), (183, 343, 228, 359), (237, 343, 324, 359), (330, 343, 376, 359), (330, 343, 376, 359), (330, 343, 376, 359), (382, 343, 408, 359), (382, 343, 408, 359), (385, 357, 428, 373), (385, 357, 428, 373), (435, 357, 464, 373), (435, 357, 464, 373), (145, 359, 171, 373), (177, 359, 271, 373), (281, 359, 299, 373), (281, 359, 299, 373), (305, 359, 335, 373), (341, 359, 377, 373), (145, 385, 195, 401), (145, 385, 195, 401), (145, 399, 179, 415), (145, 399, 179, 415), [1000, 1000, 1000, 1000], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]}"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# LayoutLM tokenizer does not support bounding boxes, so we will use the LayoutLMv2 tokenizer instead\n",
        "# Otherwise we would have to implement ourselves the mapping of bounding boxes to tokens\n",
        "# This can be tricky because some words can be split into multiple tokens\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    \"microsoft/layoutlmv2-base-uncased\"\n",
        ")\n",
        "\n",
        "# Use it like this, it can support batched inputs\n",
        "tokenizer(\n",
        "    text=train_samples[0].words,\n",
        "    boxes=train_samples[0].boxes,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TextBoxBatch:\n",
        "    words: torch.LongTensor  # (batch_size, max_seq_len)\n",
        "    boxes: torch.LongTensor  # (batch_size, max_seq_len, 4)\n",
        "    labels: torch.LongTensor  # (batch_size, max_seq_len)\n",
        "    token_type_ids: torch.LongTensor  # (batch_size, max_seq_len)\n",
        "    attention_mask: torch.LongTensor  # (batch_size, max_seq_len)\n",
        "\n",
        "    def to(self, device: str):\n",
        "        self.words = self.words.to(device)\n",
        "        self.boxes = self.boxes.to(device)\n",
        "        self.labels = self.labels.to(device)\n",
        "        self.token_type_ids = self.token_type_ids.to(device)\n",
        "        self.attention_mask = self.attention_mask.to(device)\n",
        "        return self\n",
        "\n",
        "    def __post_init__(self):\n",
        "        batch_size = self.words.shape[0]\n",
        "        assert self.words.shape == (batch_size, 512)\n",
        "        assert self.boxes.shape == (batch_size, 512, 4)\n",
        "        assert self.labels.shape == (batch_size, 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(\n",
        "    samples: list[TextBoxSample],\n",
        "    tokenizer: transformers.LayoutLMv2Tokenizer = tokenizer,\n",
        ") -> TextBoxBatch:\n",
        "    # Implement the collate_fn function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "def collate_fn(\n",
        "    samples: list[TextBoxSample],\n",
        "    tokenizer: transformers.LayoutLMv2Tokenizer = tokenizer,\n",
        ") -> TextBoxBatch:\n",
        "    encodings = tokenizer(\n",
        "        text=[sample.words for sample in samples],\n",
        "        boxes=[sample.boxes for sample in samples],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",  # return PyTorch tensors\n",
        "    )\n",
        "    encodings[\"labels\"] = (\n",
        "        torch.zeros_like(encodings[\"input_ids\"]) - 100\n",
        "    )  # -100 is the default ignore value for the loss function\n",
        "    encodings[\"labels\"][:, 0] = torch.tensor(\n",
        "        [sample.label for sample in samples], dtype=torch.long\n",
        "    )\n",
        "\n",
        "    return TextBoxBatch(\n",
        "        words=encodings[\"input_ids\"],\n",
        "        boxes=encodings[\"bbox\"],\n",
        "        labels=encodings[\"labels\"],\n",
        "        token_type_ids=encodings[\"token_type_ids\"],\n",
        "        attention_mask=encodings[\"attention_mask\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TextBoxBatch(words=tensor([[  101,  2013,  1024,  ...,     0,     0,     0],\n",
              "        [  101,  1049,  2546,  ...,     0,     0,     0],\n",
              "        [  101,  1016, 17710,  ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [  101,  1015,  1015,  ...,     0,     0,     0],\n",
              "        [  101,  2622,  3642,  ...,     0,     0,     0],\n",
              "        [  101, 25294,  2470,  ...,     0,     0,     0]]), boxes=tensor([[[  0,   0,   0,   0],\n",
              "         [145, 105, 176, 121],\n",
              "         [145, 105, 176, 121],\n",
              "         ...,\n",
              "         [  0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0]],\n",
              "\n",
              "        [[  0,   0,   0,   0],\n",
              "         [ 87,  65, 111,  77],\n",
              "         [ 87,  65, 111,  77],\n",
              "         ...,\n",
              "         [  0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0]],\n",
              "\n",
              "        [[  0,   0,   0,   0],\n",
              "         [ 56,  64,  80,  92],\n",
              "         [299,  94, 328, 117],\n",
              "         ...,\n",
              "         [  0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[  0,   0,   0,   0],\n",
              "         [ 79, 451,  91, 479],\n",
              "         [163, 451, 175, 497],\n",
              "         ...,\n",
              "         [  0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0]],\n",
              "\n",
              "        [[  0,   0,   0,   0],\n",
              "         [495, 155, 559, 173],\n",
              "         [563, 157, 599, 171],\n",
              "         ...,\n",
              "         [  0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0]],\n",
              "\n",
              "        [[  0,   0,   0,   0],\n",
              "         [190,  90, 262, 120],\n",
              "         [276,  90, 392, 120],\n",
              "         ...,\n",
              "         [  0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0]]]), labels=tensor([[   0, -100, -100,  ..., -100, -100, -100],\n",
              "        [   1, -100, -100,  ..., -100, -100, -100],\n",
              "        [   4, -100, -100,  ..., -100, -100, -100],\n",
              "        ...,\n",
              "        [   3, -100, -100,  ..., -100, -100, -100],\n",
              "        [   1, -100, -100,  ..., -100, -100, -100],\n",
              "        [   3, -100, -100,  ..., -100, -100, -100]]), token_type_ids=tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0]]), attention_mask=tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0]]))"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# If you got it right, this should work properly\n",
        "\n",
        "collate_fn(train_samples[:12])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model - LayoutLM\n",
        "\n",
        "The transformer library provides model's code and weights. We will use the weights of a [fine-tuned model](https://huggingface.co/gurvgupta/LayoutLM_rvl-cdip) on RVL-CDIP from the hub.\n",
        "Let's first download its weights and fix his mistakes so we can load the model weights.\n",
        "\n",
        "This pre-trained model was already fine-tuned on a superset of our dataset.\n",
        "We will still fine tune it for a few epochs because our final classes are different.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not path.exists(\"LayoutLM_rvl-cdip\"):\n",
        "    !git lfs install\n",
        "    !git clone https://huggingface.co/gurvgupta/LayoutLM_rvl-cdip\n",
        "    !mv LayoutLM_rvl-cdip/LayoutLM_rvl-cdip_epoch_50.pt LayoutLM_rvl-cdip/pytorch_model.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model config LayoutLMConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_2d_position_embeddings\": 1024,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"layoutlm\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.45.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file ./LayoutLM_rvl-cdip/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LayoutLMForSequenceClassification.\n",
            "\n",
            "Some weights of LayoutLMForSequenceClassification were not initialized from the model checkpoint at ./LayoutLM_rvl-cdip and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([16, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([16]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LayoutLMForSequenceClassification(\n",
              "  (layoutlm): LayoutLMModel(\n",
              "    (embeddings): LayoutLMEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (x_position_embeddings): Embedding(1024, 768)\n",
              "      (y_position_embeddings): Embedding(1024, 768)\n",
              "      (h_position_embeddings): Embedding(1024, 768)\n",
              "      (w_position_embeddings): Embedding(1024, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): LayoutLMEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x LayoutLMLayer(\n",
              "          (attention): LayoutLMAttention(\n",
              "            (self): LayoutLMSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): LayoutLMSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LayoutLMIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LayoutLMOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): LayoutLMPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers.models.layoutlm import LayoutLMForSequenceClassification\n",
        "\n",
        "model = LayoutLMForSequenceClassification.from_pretrained(\n",
        "    \"./LayoutLM_rvl-cdip\", num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n",
        ")\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;31mSignature:\u001b[0m\n",
            "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mbbox\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mattention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mposition_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mhead_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0minputs_embeds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequenceClassifierOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDocstring:\u001b[0m\n",
            "The [`LayoutLMForSequenceClassification`] forward method, overrides the `__call__` special method.\n",
            "\n",
            "<Tip>\n",
            "\n",
            "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
            "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
            "the latter silently ignores them.\n",
            "\n",
            "</Tip>\n",
            "\n",
            "Args:\n",
            "    input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
            "        Indices of input sequence tokens in the vocabulary.\n",
            "\n",
            "        Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
            "        [`PreTrainedTokenizer.__call__`] for details.\n",
            "\n",
            "        [What are input IDs?](../glossary#input-ids)\n",
            "    bbox (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*):\n",
            "        Bounding boxes of each input sequence tokens. Selected in the range `[0,\n",
            "        config.max_2d_position_embeddings-1]`. Each bounding box should be a normalized version in (x0, y0, x1, y1)\n",
            "        format, where (x0, y0) corresponds to the position of the upper left corner in the bounding box, and (x1,\n",
            "        y1) represents the position of the lower right corner. See [Overview](#Overview) for normalization.\n",
            "    attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
            "        Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`: `1` for\n",
            "        tokens that are NOT MASKED, `0` for MASKED tokens.\n",
            "\n",
            "        [What are attention masks?](../glossary#attention-mask)\n",
            "    token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
            "        Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n",
            "        1]`: `0` corresponds to a *sentence A* token, `1` corresponds to a *sentence B* token\n",
            "\n",
            "        [What are token type IDs?](../glossary#token-type-ids)\n",
            "    position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
            "        Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
            "        config.max_position_embeddings - 1]`.\n",
            "\n",
            "        [What are position IDs?](../glossary#position-ids)\n",
            "    head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
            "        Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`: `1`\n",
            "        indicates the head is **not masked**, `0` indicates the head is **masked**.\n",
            "    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
            "        Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
            "        is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n",
            "        model's internal embedding lookup matrix.\n",
            "    output_attentions (`bool`, *optional*):\n",
            "        If set to `True`, the attentions tensors of all attention layers are returned. See `attentions` under\n",
            "        returned tensors for more detail.\n",
            "    output_hidden_states (`bool`, *optional*):\n",
            "        If set to `True`, the hidden states of all layers are returned. See `hidden_states` under returned tensors\n",
            "        for more detail.\n",
            "    return_dict (`bool`, *optional*):\n",
            "        If set to `True`, the model will return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            "\n",
            "    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
            "        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
            "        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
            "        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
            "\n",
            "\n",
            "    Returns:\n",
            "        [`transformers.modeling_outputs.SequenceClassifierOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.SequenceClassifierOutput`] or a tuple of\n",
            "        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
            "        elements depending on the configuration ([`LayoutLMConfig`]) and inputs.\n",
            "\n",
            "        - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Classification (or regression if config.num_labels==1) loss.\n",
            "        - **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) -- Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
            "        - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
            "          one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
            "\n",
            "          Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
            "        - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
            "          sequence_length)`.\n",
            "\n",
            "          Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
            "          heads.\n",
            "  \n",
            "\n",
            "    Examples:\n",
            "\n",
            "    ```python\n",
            "    >>> from transformers import AutoTokenizer, LayoutLMForSequenceClassification\n",
            "    >>> import torch\n",
            "\n",
            "    >>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n",
            "    >>> model = LayoutLMForSequenceClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n",
            "\n",
            "    >>> words = [\"Hello\", \"world\"]\n",
            "    >>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n",
            "\n",
            "    >>> token_boxes = []\n",
            "    >>> for word, box in zip(words, normalized_word_boxes):\n",
            "    ...     word_tokens = tokenizer.tokenize(word)\n",
            "    ...     token_boxes.extend([box] * len(word_tokens))\n",
            "    >>> # add bounding boxes of cls + sep tokens\n",
            "    >>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n",
            "\n",
            "    >>> encoding = tokenizer(\" \".join(words), return_tensors=\"pt\")\n",
            "    >>> input_ids = encoding[\"input_ids\"]\n",
            "    >>> attention_mask = encoding[\"attention_mask\"]\n",
            "    >>> token_type_ids = encoding[\"token_type_ids\"]\n",
            "    >>> bbox = torch.tensor([token_boxes])\n",
            "    >>> sequence_label = torch.tensor([1])\n",
            "\n",
            "    >>> outputs = model(\n",
            "    ...     input_ids=input_ids,\n",
            "    ...     bbox=bbox,\n",
            "    ...     attention_mask=attention_mask,\n",
            "    ...     token_type_ids=token_type_ids,\n",
            "    ...     labels=sequence_label,\n",
            "    ... )\n",
            "\n",
            "    >>> loss = outputs.loss\n",
            "    >>> logits = outputs.logits\n",
            "    ```\n",
            "\u001b[0;31mFile:\u001b[0m      ~/Code/NeuralDocumentClassification/.venv/lib/python3.11/site-packages/transformers/models/layoutlm/modeling_layoutlm.py\n",
            "\u001b[0;31mType:\u001b[0m      method"
          ]
        }
      ],
      "source": [
        "?model.forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3880,  0.3127,  0.1283, -0.3651, -0.1338],\n",
              "        [ 0.4449, -0.3743,  0.2650, -0.7613, -0.0456]],\n",
              "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The model can be used like this\n",
        "\n",
        "data = collate_fn(train_samples[:2])\n",
        "model(\n",
        "    input_ids=data.words,\n",
        "    bbox=data.boxes,\n",
        "    token_type_ids=data.token_type_ids,\n",
        "    attention_mask=data.attention_mask,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model\n",
        "\n",
        "First, let's copy the training loop procedure from the previous notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copied from `chapter_1_vision.ipynb`\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    device: torch.device,\n",
        ") -> float:\n",
        "    \"\"\"This function should train the model for one epoch and return the average loss\"\"\"\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    with tqdm.tqdm(desc=\"Training\", total=len(dataloader)) as pbar:\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            images, labels = batch.images.to(device), batch.labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "            outputs = model(images)  # Compute model's predictions\n",
        "            loss = loss_fn(outputs, labels)  # Compute the loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            pbar.set_postfix(loss=epoch_loss / (i + 1))\n",
        "            pbar.update(1)\n",
        "    mean_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Training loss (↓): {mean_loss:.4f}\")\n",
        "    return mean_loss\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    metric_fn: nn.Module,\n",
        "    device: torch.device,\n",
        "    dataset_name: str = \"validation\",\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"This function should evaluate the model on the dataset and return the average loss and metric\"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    epoch_metric = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(dataloader, desc=\"Evaluation\"):\n",
        "            images, labels = batch.images.to(device), batch.labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            metric = metric_fn(outputs.argmax(dim=-1), labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_metric += metric.item()\n",
        "\n",
        "        mean_loss = epoch_loss / len(dataloader)\n",
        "        print(f\"{dataset_name.capitalize()} loss (↓): {mean_loss:.4f}\")\n",
        "        mean_metric = epoch_metric / len(dataloader)\n",
        "        print(f\"{dataset_name.capitalize()} metric (↑): {mean_metric:.4f}\")\n",
        "        return mean_loss, mean_metric\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_dataloader: data.DataLoader,\n",
        "    validation_dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    metric_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    device: torch.device,\n",
        "    n_epochs: int = 10,\n",
        ") -> tuple[list[float], list[float], list[float]]:\n",
        "    \"\"\"This function should train the model for some epochs and return the training and validation losses\"\"\"\n",
        "    train_losses = []\n",
        "    validation_losses = []\n",
        "    validation_metrics = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
        "        train_loss = train_one_epoch(\n",
        "            model, train_dataloader, loss_fn, optimizer, device\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        validation_loss, validation_metric = evaluate(\n",
        "            model, validation_dataloader, loss_fn, metric_fn, device\n",
        "        )\n",
        "        validation_losses.append(validation_loss)\n",
        "        validation_metrics.append(validation_metric)\n",
        "\n",
        "    return train_losses, validation_losses, validation_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "    DocumentTextBoxDataset(train_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        ")\n",
        "validation_loader = data.DataLoader(\n",
        "    DocumentTextBoxDataset(validation_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "device = torch.device(\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "selected_model = mlp_model\n",
        "\n",
        "optimizer = torch.optim.Adam(selected_model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "metric_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "n_epochs = 5\n",
        "\n",
        "hist = train(\n",
        "    selected_model,\n",
        "    train_loader,\n",
        "    validation_loader,\n",
        "    loss_fn,\n",
        "    metric_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    n_epochs=n_epochs,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "skeleton_ocr.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "orig_nbformat": 2,
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
