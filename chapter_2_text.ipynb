{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thibaultdouzon/NeuralDocumentClassification/blob/master/chapter_2_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56cA2WtUQZ-I"
      },
      "source": [
        "# Training a classifier on OCR text input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAd75X5JT4vI"
      },
      "source": [
        "# Imports & Cloning repository\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NSkdjetznvp"
      },
      "source": [
        "You will need this library version for the next of the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erzq08Znznvp"
      },
      "outputs": [],
      "source": [
        "%pip install transformers==4.45.1\n",
        "%pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVIlDgNHW62j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from os import path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nOYey20znvq"
      },
      "outputs": [],
      "source": [
        "class_names = [\"email\", \"form\", \"handwritten\", \"invoice\", \"advertisement\"]\n",
        "NUM_CLASSES = len(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHSxMpusznvq"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"NeuralDocumentClassification\"):\n",
        "    !git clone https://github.com/thibaultdouzon/NeuralDocumentClassification.git\n",
        "else:\n",
        "    !git -C NeuralDocumentClassification pull\n",
        "sys.path.append(\"NeuralDocumentClassification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fu_jZ34oznvq"
      },
      "outputs": [],
      "source": [
        "from src import download_dataset\n",
        "\n",
        "dataset_path = \"dataset\"\n",
        "download_dataset.download_and_extract(\"all\", dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFTRMvR8znvq"
      },
      "outputs": [],
      "source": [
        "with open(path.join(dataset_path, \"train.pkl\"), \"rb\") as f:\n",
        "    train_dataset = pickle.load(f)\n",
        "\n",
        "with open(path.join(dataset_path, \"test.pkl\"), \"rb\") as f:\n",
        "    test_dataset = pickle.load(f)\n",
        "\n",
        "with open(path.join(dataset_path, \"validation.pkl\"), \"rb\") as f:\n",
        "    validation_dataset = pickle.load(f)\n",
        "\n",
        "\n",
        "for split_name, split_dataset in zip(\n",
        "    [\"train\", \"test\", \"validation\"], [train_dataset, test_dataset, validation_dataset]\n",
        "):\n",
        "    print(f\"{split_name}_dataset contains {len(split_dataset)} documents\")\n",
        "train_dataset[0].keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K-0IUpPznvq"
      },
      "source": [
        "Each `dataset` object is a `list` containing multiple document information. A document is a `dict` with the following structure:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"id\": \"Unique document identifier\",\n",
        "  \"image\": \"A PIL.Image object containing the document's image\",\n",
        "  \"label\": \"A number between in [0 .. 4] representing the class of the document\",\n",
        "  \"words\": \"A list of strings (not words !) extracted from the image with an OCR\",\n",
        "  \"boxes\": \"A list of tuples of numbers providing the position of each word in the document\"\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uefjUmcMRnLT"
      },
      "source": [
        "# Explore the data\n",
        "\n",
        "Take the time to explore the textual data included in the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKHd08xTznvq"
      },
      "source": [
        "Ideas\n",
        "\n",
        "- 10 most common words? (hint: Counter)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgC5wCAVznvr"
      },
      "outputs": [],
      "source": [
        "### Insert your code here ###\n",
        "# See the expected solution by clicking on the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "M6ibhOtRznvr"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "all_texts = [\n",
        "    [word for sentence in doc[\"words\"] for word in sentence.split()]\n",
        "    for doc in validation_dataset + test_dataset + train_dataset\n",
        "]\n",
        "\n",
        "most_common_words = Counter([w for text in all_texts for w in text])\n",
        "most_common_words.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3uy2Y5Sznvr"
      },
      "source": [
        "- Count number of unique words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnAj1L7hznvr"
      },
      "outputs": [],
      "source": [
        "### Insert your code here ###\n",
        "# See the expected solution by clicking on the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "O7v-BCOyznvr"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "n_unique_words = len({w for text in all_texts for w in text})\n",
        "n_unique_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOzDRq7wznvr"
      },
      "source": [
        "- Distribution of words (cumulative occurences plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x63Gag1dznvr"
      },
      "outputs": [],
      "source": [
        "### Insert your code here ###\n",
        "# See the expected solution by clicking on the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zVoqjmHzznvr"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "# Zipf's law\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(\n",
        "    [c / sum(most_common_words.values()) for w, c in most_common_words.most_common(50)]\n",
        ")\n",
        "\n",
        "# put words on xlabel\n",
        "plt.xticks(\n",
        "    range(50),\n",
        "    [w for w, c in most_common_words.most_common(50)],\n",
        "    rotation=80,\n",
        "    fontsize=9,\n",
        ")\n",
        "plt.ylabel(\"Word frequency\")\n",
        "plt.title(\"Word frequency in the dataset\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4YiudTfAznvr"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from itertools import accumulate\n",
        "\n",
        "cum_word_occurences = list(\n",
        "    accumulate([count for word, count in most_common_words.most_common(n_unique_words)])\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(cum_word_occurences)\n",
        "\n",
        "plt.xlabel(\"Rank of the word\")\n",
        "plt.ylabel(\"Number of occurences\")\n",
        "plt.title(\"Cumulative number of occurences of the most common words\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNr1m0q_Tjo9"
      },
      "source": [
        "# Classification with Scikit Learn\n",
        "\n",
        "In this part, we will train simple classification algorithms using Scikit-learn library.\n",
        "The following code defines the training samples we will use.\n",
        "\n",
        "You can try to modify it to further clean the data using the nltk library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WheOgS6TW62r"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import sklearn\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TextSample:\n",
        "    text: str\n",
        "    label: int\n",
        "\n",
        "    def __init__(self, document: dict):\n",
        "        self.text = \" \".join(\n",
        "            [word for sentence in document[\"words\"] for word in sentence.split()]\n",
        "        )\n",
        "        self.label = document[\"label\"]\n",
        "\n",
        "\n",
        "train_samples = [TextSample(doc) for doc in train_dataset]\n",
        "\n",
        "test_samples = [TextSample(doc) for doc in test_dataset]\n",
        "\n",
        "validation_samples = [TextSample(doc) for doc in validation_dataset]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43A6uS2Oznvr"
      },
      "source": [
        "## Tokenization and Vectorization\n",
        "\n",
        "To train models at solving our problem, we need to convert texts into vectors that will represent our documents.\n",
        "Take a look at Scikit Learn [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#) and [TFIDFVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
        "First fit a vectorizer on the training set, then apply the vectorization transformation to each dataset split.\n",
        "\n",
        "What are the shapes of the resulting vectors? What does each dimension mean?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngR65gEjW62w"
      },
      "outputs": [],
      "source": [
        "### Insert your code here ###\n",
        "# See the expected solution by clicking on the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2t-OIyq_znvr"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform([sample.text for sample in train_samples])\n",
        "X_test = vectorizer.transform([sample.text for sample in test_samples])\n",
        "X_validation = vectorizer.transform([sample.text for sample in validation_samples])\n",
        "\n",
        "Y_train = [sample.label for sample in train_samples]\n",
        "Y_test = [sample.label for sample in test_samples]\n",
        "Y_validation = [sample.label for sample in validation_samples]\n",
        "\n",
        "X_train.shape, X_test.shape, X_validation.shape\n",
        "# Each vector's first dimension is the number of documents, the second dimension is the number of unique words in the dataset\n",
        "# The value at (i, j) is the number of occurences of the j-th word in the i-th document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFHy4rtVW63D"
      },
      "source": [
        "## Basic Model: Scikit-Learn Classification\n",
        "\n",
        "Use any Scikit-Learn classification model to train a first text model.\n",
        "Good first picks: [Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) or [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dj7M6rO-znvr"
      },
      "outputs": [],
      "source": [
        "### Insert your code here ###\n",
        "# See the expected solution by clicking on the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "M8QN3cGjznvs"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(kernel=\"linear\")\n",
        "model.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUpI0Q_rznvs"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "Use Scikit-Learn [metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) to evaluate your model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp4DRHeZznvs"
      },
      "outputs": [],
      "source": [
        "### Insert your code here ###\n",
        "# See the expected solution by clicking on the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "omWk_5iwznvs"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "print(\"Test\")\n",
        "Y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(f\"Accuracy on the test set: {accuracy:.2f}\")\n",
        "print(confusion_matrix(Y_test, Y_pred))\n",
        "\n",
        "print(\"Validation\")\n",
        "Y_pred = model.predict(X_validation)\n",
        "accuracy = accuracy_score(Y_validation, Y_pred)\n",
        "print(f\"Accuracy on the validation set: {accuracy:.2f}\")\n",
        "print(confusion_matrix(Y_validation, Y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoTPVaKYW63Q"
      },
      "source": [
        "# Transformers\n",
        "\n",
        "Done playing with kid toys.\n",
        "\n",
        "All modern AI models use the [Transformer architecture](https://arxiv.org/pdf/1706.03762). The initial research paper is one of the most influencial of the last decade.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7kDysEJznvs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from torch import nn\n",
        "from torch.utils import data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLeQQjJUznvs"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "Transformers usually use subword tokenizer, ie. a word _can_ be tokenized into multiple tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN3QZJaAbN6-"
      },
      "outputs": [],
      "source": [
        "# Let's use LayoutLM tokenizer first\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    \"microsoft/layoutlm-base-uncased\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNXkHQv6znvs"
      },
      "outputs": [],
      "source": [
        "encoding = tokenizer(\"Hello, world! I can tokenize any sentence.\")\n",
        "\n",
        "for token_id in encoding[\"input_ids\"]:\n",
        "    print(tokenizer.decode(token_id))\n",
        "\n",
        "# Note how `tokenize` is encoded as `token ##ize`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bPqgV6Iznvs"
      },
      "source": [
        "## Dataset for LayoutLM\n",
        "\n",
        "LayoutLM uses both textual and 2D positional information, here is a new data sample class to work with\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zidLChIznvs"
      },
      "outputs": [],
      "source": [
        "def split_sentence_into_words(\n",
        "    sentence: str, sentence_box: list[int, int, int, int]\n",
        ") -> tuple[list[str], list[tuple[int, int, int, int]]]:\n",
        "    ret_words = []\n",
        "    ret_boxes = []\n",
        "    words = sentence.split()\n",
        "\n",
        "    ret_words.extend(words)\n",
        "\n",
        "    words_len = [len(word) for word in words]\n",
        "    box_width = sentence_box[2] - sentence_box[0]\n",
        "\n",
        "    word_left = sentence_box[0]\n",
        "    for word_len in words_len:\n",
        "        word_right = word_left + int(word_len * box_width / len(sentence))\n",
        "        ret_boxes.append((word_left, sentence_box[1], word_right, sentence_box[3]))\n",
        "        word_left = word_right + int(1 * box_width / len(sentence))\n",
        "\n",
        "    return ret_words, ret_boxes\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TextBoxSample:\n",
        "    words: list[str]\n",
        "    boxes: list[tuple[int, int, int, int]]  # (left, top, right, bottom)\n",
        "    label: int\n",
        "\n",
        "    def __init__(self, document: dict):\n",
        "        self.words = []\n",
        "        self.boxes = []\n",
        "\n",
        "        # We need to split the words in the sentences and compute the bounding boxes for each word\n",
        "        for sentence, sentence_box in zip(document[\"words\"], document[\"boxes\"]):\n",
        "            new_words, new_boxes = split_sentence_into_words(sentence, sentence_box)\n",
        "            self.words.extend(new_words)\n",
        "            self.boxes.extend(new_boxes)\n",
        "\n",
        "        self.label = document[\"label\"]\n",
        "\n",
        "\n",
        "train_samples = [TextBoxSample(doc) for doc in train_dataset]\n",
        "\n",
        "test_samples = [TextBoxSample(doc) for doc in test_dataset]\n",
        "\n",
        "validation_samples = [TextBoxSample(doc) for doc in validation_dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FayLGWLdznvs"
      },
      "source": [
        "Let's implement the pytorch dataset that will hold those samples. Keep it very simple, we will delay most computation to the batching function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWQfF3oUznvt"
      },
      "outputs": [],
      "source": [
        "### Modify the code here ###\n",
        "# See the expected solution by clicking on the cell below\n",
        "\n",
        "class DocumentTextBoxDataset(data.Dataset):\n",
        "    def __init__(self, samples: list[TextBoxSample]):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __getitem__(self, idx: int) -> TextBoxSample:\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "s7T2tpA1znvt"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "class DocumentTextBoxDataset(data.Dataset):\n",
        "    def __init__(self, samples: list[TextBoxSample]):\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx) -> TextBoxSample:\n",
        "        return self.samples[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Nx506A9znvt"
      },
      "source": [
        "## Batching function\n",
        "\n",
        "Like we did in the vision part, we need to implement a batching function that will batch together multiple inputs together and prepare them to be fed to the model.\n",
        "\n",
        "Huggingface transformers tokenizers have the hability to tokenize a whole batch at once and perform most of the computation for us.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipXBMTPwznvt"
      },
      "outputs": [],
      "source": [
        "# LayoutLM tokenizer does not support bounding boxes, so we will use the LayoutLMv2 tokenizer instead\n",
        "# Otherwise we would have to implement ourselves the mapping of bounding boxes to tokens\n",
        "# This can be tricky because some words can be split into multiple tokens\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    \"microsoft/layoutlmv2-base-uncased\"\n",
        ")\n",
        "\n",
        "# Use it like this, it can support batched inputs\n",
        "tokenizer(\n",
        "    text=train_samples[0].words,\n",
        "    boxes=train_samples[0].boxes,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AeSC__pznvt"
      },
      "source": [
        "This is the definition we will use for a batch of samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRfVvgPqznvt"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TextBoxBatch:\n",
        "    words: torch.LongTensor  # (batch_size, max_seq_len)\n",
        "    boxes: torch.LongTensor  # (batch_size, max_seq_len, 4)\n",
        "    labels: torch.LongTensor  # (batch_size)\n",
        "    token_type_ids: torch.LongTensor  # (batch_size, max_seq_len)\n",
        "    attention_mask: torch.LongTensor  # (batch_size, max_seq_len)\n",
        "\n",
        "    def to(self, device: str):\n",
        "        self.words = self.words.to(device)\n",
        "        self.boxes = self.boxes.to(device)\n",
        "        self.labels = self.labels.to(device)\n",
        "        self.token_type_ids = self.token_type_ids.to(device)\n",
        "        self.attention_mask = self.attention_mask.to(device)\n",
        "        return self\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.boxes.max() > 1000 or self.boxes.min() < 0:\n",
        "            self.boxes.clamp_(min=0, max=1000)\n",
        "\n",
        "        self.boxes[:, :, 0] = torch.where(\n",
        "            self.boxes[:, :, 0] > self.boxes[:, :, 2],\n",
        "            self.boxes[:, :, 2],\n",
        "            self.boxes[:, :, 0]\n",
        "        )\n",
        "        self.boxes[:, :, 1] = torch.where(\n",
        "            self.boxes[:, :, 1] > self.boxes[:, :, 3],\n",
        "            self.boxes[:, :, 3],\n",
        "            self.boxes[:, :, 1]\n",
        "        )\n",
        "\n",
        "        self.boxes.clamp_(min=0, max=1000)\n",
        "        batch_size = self.words.shape[0]\n",
        "        assert self.words.shape == (batch_size, 512)\n",
        "        assert self.boxes.shape == (batch_size, 512, 4)\n",
        "        assert self.labels.shape == (batch_size,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIS1OyHuznvt"
      },
      "source": [
        "Implement the batching function that converts a list of `TextBoxSample` to a `TextBoxBatch`. Use the tokenizer to tokenize words and boxes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9XLlwtYznvt"
      },
      "outputs": [],
      "source": [
        "### Modify the code here ###\n",
        "# See the expected solution by clicking on the cell below\n",
        "\n",
        "def collate_fn(\n",
        "    samples: list[TextBoxSample],\n",
        "    tokenizer: transformers.LayoutLMv2Tokenizer = tokenizer,\n",
        ") -> TextBoxBatch:\n",
        "    # Implement the collate_fn function\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6KDM1TT2znvt"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "def collate_fn(\n",
        "    samples: list[TextBoxSample],\n",
        "    tokenizer: transformers.LayoutLMv2Tokenizer = tokenizer,\n",
        ") -> TextBoxBatch:\n",
        "    encodings = tokenizer(\n",
        "        text=[sample.words for sample in samples],\n",
        "        boxes=[sample.boxes for sample in samples],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",  # return PyTorch tensors\n",
        "    )\n",
        "    encodings[\"labels\"] = torch.tensor(\n",
        "        [sample.label for sample in samples], dtype=torch.long\n",
        "    )\n",
        "\n",
        "    return TextBoxBatch(\n",
        "        words=encodings[\"input_ids\"],\n",
        "        boxes=encodings[\"bbox\"],\n",
        "        labels=encodings[\"labels\"],\n",
        "        token_type_ids=encodings[\"token_type_ids\"],\n",
        "        attention_mask=encodings[\"attention_mask\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA7bClxMznvt"
      },
      "outputs": [],
      "source": [
        "# If you got it right, this should work properly\n",
        "\n",
        "collate_fn(train_samples[:12])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCppSL0Lznvt"
      },
      "source": [
        "## Model - LayoutLM\n",
        "\n",
        "The transformer library provides model's code and weights. We will use the weights of a [fine-tuned model](https://huggingface.co/gurvgupta/LayoutLM_rvl-cdip) on RVL-CDIP from the hub.\n",
        "Let's first download its weights and fix his mistakes so we can load the model weights.\n",
        "\n",
        "This pre-trained model was already fine-tuned on a superset of our dataset.\n",
        "We will still fine tune it for a few epochs because our final classes are different.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJbbTeSbznvt"
      },
      "outputs": [],
      "source": [
        "if not path.exists(\"LayoutLM_rvl-cdip\"):\n",
        "    !git lfs install\n",
        "    !git clone https://huggingface.co/gurvgupta/LayoutLM_rvl-cdip\n",
        "    !mv LayoutLM_rvl-cdip/LayoutLM_rvl-cdip_epoch_50.pt LayoutLM_rvl-cdip/pytorch_model.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U77Hj74znvt"
      },
      "outputs": [],
      "source": [
        "from transformers.models.layoutlm import LayoutLMForSequenceClassification\n",
        "\n",
        "model = LayoutLMForSequenceClassification.from_pretrained(\n",
        "    \"./LayoutLM_rvl-cdip\", num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n",
        ")\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2_GS2csznvt"
      },
      "source": [
        "The model can be used like this, observe it's inputs and output type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3v2wJxzznvt"
      },
      "outputs": [],
      "source": [
        "# The model can be used like this\n",
        "\n",
        "batch = collate_fn(train_samples[:2])\n",
        "model(\n",
        "    input_ids=batch.words,\n",
        "    bbox=batch.boxes,\n",
        "    token_type_ids=batch.token_type_ids,\n",
        "    attention_mask=batch.attention_mask,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJjjZHmjznvt"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "First, let's copy the training loop procedure from the previous notebook and modify it to adapt to the new data format and model's output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGtt5mXuznvt"
      },
      "outputs": [],
      "source": [
        "### Insert your code here ###\n",
        "# See the expected solution by clicking on the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "21jxrt7wznvt"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Copied from `chapter_1_vision.ipynb`\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    device: torch.device,\n",
        ") -> float:\n",
        "    \"\"\"This function should train the model for one epoch and return the average loss\"\"\"\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    with tqdm.tqdm(desc=\"Training\", total=len(dataloader)) as pbar:\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            batch.to(device)\n",
        "            words, boxes, labels, token_type_ids, attention_mask = (\n",
        "                batch.words,\n",
        "                batch.boxes,\n",
        "                batch.labels,\n",
        "                batch.token_type_ids,\n",
        "                batch.attention_mask,\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "            outputs = model(\n",
        "                input_ids=words,\n",
        "                bbox=boxes,\n",
        "                token_type_ids=token_type_ids,\n",
        "                attention_mask=attention_mask,\n",
        "            ).logits  # Compute model's predictions\n",
        "\n",
        "            loss = loss_fn(outputs, labels)  # Compute the loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            pbar.set_postfix(loss=epoch_loss / (i + 1))\n",
        "            pbar.update(1)\n",
        "    mean_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Training loss (↓): {mean_loss:.4f}\")\n",
        "    return mean_loss\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    metric_fn: nn.Module,\n",
        "    device: torch.device,\n",
        "    dataset_name: str = \"validation\",\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"This function should evaluate the model on the dataset and return the average loss and metric\"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    epoch_metric = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(dataloader, desc=\"Evaluation\"):\n",
        "            batch.to(device)\n",
        "            words, boxes, labels, token_type_ids, attention_mask = (\n",
        "                batch.words,\n",
        "                batch.boxes,\n",
        "                batch.labels,\n",
        "                batch.token_type_ids,\n",
        "                batch.attention_mask,\n",
        "            )\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=words,\n",
        "                bbox=boxes,\n",
        "                token_type_ids=token_type_ids,\n",
        "                attention_mask=attention_mask,\n",
        "            ).logits  # Compute model's predictions\n",
        "\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            metric = metric_fn(outputs.argmax(dim=-1), labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_metric += metric.item()\n",
        "\n",
        "        mean_loss = epoch_loss / len(dataloader)\n",
        "        print(f\"{dataset_name.capitalize()} loss (↓): {mean_loss:.4f}\")\n",
        "        mean_metric = epoch_metric / len(dataloader)\n",
        "        print(f\"{dataset_name.capitalize()} metric (↑): {mean_metric:.4f}\")\n",
        "        return mean_loss, mean_metric\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_dataloader: data.DataLoader,\n",
        "    validation_dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    metric_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    device: torch.device,\n",
        "    n_epochs: int = 10,\n",
        ") -> tuple[list[float], list[float], list[float]]:\n",
        "    \"\"\"This function should train the model for some epochs and return the training and validation losses\"\"\"\n",
        "    train_losses = []\n",
        "    validation_losses = []\n",
        "    validation_metrics = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
        "        train_loss = train_one_epoch(\n",
        "            model, train_dataloader, loss_fn, optimizer, device\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        validation_loss, validation_metric = evaluate(\n",
        "            model, validation_dataloader, loss_fn, metric_fn, device\n",
        "        )\n",
        "        validation_losses.append(validation_loss)\n",
        "        validation_metrics.append(validation_metric)\n",
        "\n",
        "    return train_losses, validation_losses, validation_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaP58wXLznvu"
      },
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "    DocumentTextBoxDataset(train_samples),\n",
        "    batch_size=8,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        ")\n",
        "validation_loader = data.DataLoader(\n",
        "    DocumentTextBoxDataset(validation_samples),\n",
        "    batch_size=8,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "device = torch.device(\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "selected_model = model\n",
        "\n",
        "optimizer = torch.optim.Adam(selected_model.parameters(), lr=1e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "metric_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "n_epochs = 2\n",
        "\n",
        "hist = train(\n",
        "    selected_model,\n",
        "    train_loader,\n",
        "    validation_loader,\n",
        "    loss_fn,\n",
        "    metric_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    n_epochs=n_epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do not hesitate to compare the performance of the models of the first and second notebook."
      ],
      "metadata": {
        "id": "8bRYtgPS0Dc0"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "skeleton_ocr.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "file_extension": ".py",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "orig_nbformat": 2,
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
