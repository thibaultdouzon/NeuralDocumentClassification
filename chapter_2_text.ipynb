{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clemsage/NeuralDocumentClassification/blob/master/skeleton_ocr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "56cA2WtUQZ-I"
      },
      "source": [
        "# Training a classifier on OCR text input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AAd75X5JT4vI"
      },
      "source": [
        "# Imports & Cloning repository\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mVIlDgNHW62j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from os import path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_names = [\"email\", \"form\", \"handwritten\", \"invoice\", \"advertisement\"]\n",
        "NUM_CLASSES = len(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists(\"NeuralDocumentClassification\"):\n",
        "    !git clone https://github.com/thibaultdouzon/NeuralDocumentClassification.git\n",
        "else:\n",
        "    !git -C NeuralDocumentClassification pull\n",
        "sys.path.append(\"NeuralDocumentClassification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src import download_dataset\n",
        "\n",
        "dataset_path = \"dataset\"\n",
        "\n",
        "download_dataset.download_and_extract(\"all\", dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(path.join(dataset_path, \"train.pkl\"), \"rb\") as f:\n",
        "    train_dataset = pickle.load(f)\n",
        "\n",
        "with open(path.join(dataset_path, \"test.pkl\"), \"rb\") as f:\n",
        "    test_dataset = pickle.load(f)\n",
        "\n",
        "with open(path.join(dataset_path, \"validation.pkl\"), \"rb\") as f:\n",
        "    validation_dataset = pickle.load(f)\n",
        "\n",
        "\n",
        "for split_name, split_dataset in zip(\n",
        "    [\"train\", \"test\", \"validation\"], [train_dataset, test_dataset, validation_dataset]\n",
        "):\n",
        "    print(f\"{split_name}_dataset contains {len(split_dataset)} documents\")\n",
        "train_dataset[0].keys()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each `dataset` object is a `list` containing multiple document information. A document is a `dict` with the following structure:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"id\": \"Unique document identifier\",\n",
        "  \"image\": \"A PIL.Image object containing the document's image\",\n",
        "  \"label\": \"A number between in [0 .. 4] representing the class of the document\",\n",
        "  \"words\": \"A list of strings (not words !) extracted from the image with an OCR\",\n",
        "  \"boxes\": \"A list of tuples of numbers providing the position of each word in the document\"\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uefjUmcMRnLT"
      },
      "source": [
        "# Explore the data\n",
        "\n",
        "Take the time to explore the textual data included in the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ideas\n",
        "\n",
        "- 10 most common words? (hint: Counter)\n",
        "- Count number of unique words\n",
        "- Distribution of words (cumulative occurences plot)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "all_texts = [\n",
        "    [word for sentence in doc[\"words\"] for word in sentence.split()]\n",
        "    for doc in validation_dataset + test_dataset + train_dataset\n",
        "]\n",
        "\n",
        "most_common_words = Counter([w for text in all_texts for w in text])\n",
        "most_common_words.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "n_unique_words = len({w for text in all_texts for w in text})\n",
        "n_unique_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "# Zipf's law\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(\n",
        "    [c / sum(most_common_words.values()) for w, c in most_common_words.most_common(50)]\n",
        ")\n",
        "\n",
        "# put words on xlabel\n",
        "plt.xticks(\n",
        "    range(50),\n",
        "    [w for w, c in most_common_words.most_common(50)],\n",
        "    rotation=80,\n",
        "    fontsize=9,\n",
        ")\n",
        "plt.ylabel(\"Word frequency\")\n",
        "plt.title(\"Word frequency in the dataset\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "from itertools import accumulate\n",
        "\n",
        "cum_word_occurences = list(\n",
        "    accumulate([count for word, count in most_common_words.most_common(n_unique_words)])\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(cum_word_occurences)\n",
        "\n",
        "plt.xlabel(\"Rank of the word\")\n",
        "plt.ylabel(\"Number of occurences\")\n",
        "plt.title(\"Cumulative number of occurences of the most common words\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KNr1m0q_Tjo9"
      },
      "source": [
        "# Classification with Scikit Learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WheOgS6TW62r"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import sklearn\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TextSample:\n",
        "    text: str\n",
        "    label: int\n",
        "\n",
        "    def __init__(self, document: dict):\n",
        "        self.text = \" \".join(\n",
        "            [word for sentence in document[\"words\"] for word in sentence.split()]\n",
        "        )\n",
        "        self.label = document[\"label\"]\n",
        "\n",
        "\n",
        "train_samples = [TextSample(doc) for doc in train_dataset]\n",
        "\n",
        "test_samples = [TextSample(doc) for doc in test_dataset]\n",
        "\n",
        "validation_samples = [TextSample(doc) for doc in validation_dataset]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization and Vectorization\n",
        "\n",
        "To train models at solving our problem, we need to convert texts into vectors that will represent our documents.\n",
        "Take a look at Scikit Learn [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#) and [TFIDFVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
        "First fit a vectorizer on the training set, then apply the vectorization transformation to each dataset split.\n",
        "\n",
        "What are the shapes of the resulting vectors? What does each dimension mean?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ngR65gEjW62w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform([sample.text for sample in train_samples])\n",
        "X_test = vectorizer.transform([sample.text for sample in test_samples])\n",
        "X_validation = vectorizer.transform([sample.text for sample in validation_samples])\n",
        "\n",
        "Y_train = [sample.label for sample in train_samples]\n",
        "Y_test = [sample.label for sample in test_samples]\n",
        "Y_validation = [sample.label for sample in validation_samples]\n",
        "\n",
        "\n",
        "X_train.shape, X_test.shape, X_validation.shape\n",
        "# Each vector's first dimension is the number of documents, the second dimension is the number of unique words in the dataset\n",
        "# The value at (i, j) is the number of occurences of the j-th word in the i-th document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FFHy4rtVW63D"
      },
      "source": [
        "## Basic Model: Scikit-Learn Classification\n",
        "\n",
        "Use any Scikit-Learn classification model to train a first text model.\n",
        "Good first picks: [Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) or [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(kernel=\"linear\")\n",
        "model.fit(X_train, Y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the model\n",
        "\n",
        "Use Scikit-Learn [metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) to evaluate your model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "print(\"Test\")\n",
        "Y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(f\"Accuracy on the test set: {accuracy:.2f}\")\n",
        "print(confusion_matrix(Y_test, Y_pred))\n",
        "\n",
        "print(\"Validation\")\n",
        "Y_pred = model.predict(X_validation)\n",
        "accuracy = accuracy_score(Y_validation, Y_pred)\n",
        "print(f\"Accuracy on the validation set: {accuracy:.2f}\")\n",
        "print(confusion_matrix(Y_validation, Y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QoTPVaKYW63Q"
      },
      "source": [
        "# Transformers\n",
        "\n",
        "Done playing with kids toys.\n",
        "\n",
        "All modern AI models use the [Transformer architecture](https://arxiv.org/pdf/1706.03762). The initial research paper is one of the most influencial of the last decade.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization\n",
        "\n",
        "Transformers usually use subword tokenizer, ie. a word _can_ be tokenized into multiple tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YN3QZJaAbN6-"
      },
      "outputs": [],
      "source": [
        "# Let's use LayoutLM tokenizer first\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    \"microsoft/layoutlm-base-uncased\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoding = tokenizer(\"Hello, world! I can tokenize any sentence.\")\n",
        "\n",
        "for token_id in encoding[\"input_ids\"]:\n",
        "    print(tokenizer.decode(token_id))\n",
        "\n",
        "# Note how `tokenize` is encoded as `token ##ize`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset for LayoutLM\n",
        "\n",
        "LayoutLM uses both textual and 2D positional information, here is a new data sample class to work with\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TextBoxSample:\n",
        "    words: list[str]\n",
        "    boxes: list[tuple[int, int, int, int]]  # (left, top, right, bottom)\n",
        "    label: int\n",
        "\n",
        "    def __init__(self, document: dict):\n",
        "        self.words = []\n",
        "        self.boxes = []\n",
        "\n",
        "        # We need to split the words in the sentences and compute the bounding boxes for each word\n",
        "        for sentence, sentence_box in zip(document[\"words\"], document[\"boxes\"]):\n",
        "            words = sentence.split()\n",
        "            self.words.extend(words)\n",
        "\n",
        "            words_len = [len(word) for word in words]\n",
        "            box_width = sentence_box[2] - sentence_box[0]\n",
        "\n",
        "            word_left = sentence_box[0]\n",
        "            for word_len in words_len:\n",
        "                word_right = word_left + int(word_len * box_width / len(sentence))\n",
        "                self.boxes.append(\n",
        "                    (word_left, sentence_box[1], word_right, sentence_box[3])\n",
        "                )\n",
        "                word_left = word_right + int(1 * box_width / len(sentence))\n",
        "\n",
        "        self.label = document[\"label\"]\n",
        "\n",
        "\n",
        "train_samples = [TextBoxSample(doc) for doc in train_dataset]\n",
        "\n",
        "test_samples = [TextBoxSample(doc) for doc in test_dataset]\n",
        "\n",
        "validation_samples = [TextBoxSample(doc) for doc in validation_dataset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LayoutLM tokenizer does not support bounding boxes, so we will use the LayoutLMv2 tokenizer instead\n",
        "# Otherwise we would have to implement ourselves the mapping of bounding boxes to tokens\n",
        "# This can be tricky because some words can be split into multiple tokens\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    \"microsoft/layoutlmv2-base-uncased\"\n",
        ")\n",
        "\n",
        "# Use it like this, it can support batched inputs\n",
        "tokenizer(\n",
        "    text=train_samples[0].words, boxes=train_samples[0].boxes, padding=\"max_length\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batching function\n",
        "\n",
        "Like we did in th vision part, we need to implement a batching function that will batch together multiple inputs together and prepare them to be fed to the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "def collate_fn(\n",
        "    samples: list[TextBoxSample],\n",
        "    tokenizer: transformers.LayoutLMv2Tokenizer = tokenizer,\n",
        "):\n",
        "    encodings = tokenizer(\n",
        "        text=[sample.words for sample in samples],\n",
        "        boxes=[sample.boxes for sample in samples],\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",  # return PyTorch tensors\n",
        "    )\n",
        "    encoding[\"labels\"] = (\n",
        "        torch.zeros_like(encoding[\"input_ids\"]) - 100\n",
        "    )  # -100 is the default ignore value for the loss function\n",
        "    encoding[\"labels\"][:, 0] = torch.tensor(\n",
        "        [sample.label for sample in samples], dtype=torch.long\n",
        "    )\n",
        "\n",
        "    return encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model\n",
        "\n",
        "The transformer library provides model's code and weights. We will use the weights of a fine-tuned model on RVL-CDIP from the hub\n",
        "Let's first download its weigits and fix his mistakes so we can load the model weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/gurvgupta/LayoutLM_rvl-cdip\n",
        "!mv LayoutLM_rvl-cdip/LayoutLM_rvl-cdip_epoch_50.pt LayoutLM_rvl-cdip/pytorch_model.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers.models.layoutlm import LayoutLMForSequenceClassification\n",
        "\n",
        "model = LayoutLMForSequenceClassification.from_pretrained(\n",
        "    \"./LayoutLM_rvl-cdip\", num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "skeleton_ocr.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "orig_nbformat": 2,
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
