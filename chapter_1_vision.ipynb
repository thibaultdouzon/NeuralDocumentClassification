{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thibaultdouzon/NeuralDocumentClassification/blob/master/chapter_1_vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j5UeZiGlvNoH"
      },
      "source": [
        "# Setting up the computing environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n52k6VoU1brz"
      },
      "source": [
        "## Install and import PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting click\n",
            "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting gdown\n",
            "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: ipython in ./.venv/lib/python3.11/site-packages (8.28.0)\n",
            "Requirement already satisfied: jupyter in ./.venv/lib/python3.11/site-packages (1.1.1)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.9.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.1.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pillow\n",
            "  Downloading pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
            "Collecting polars\n",
            "  Downloading polars-1.9.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (14 kB)\n",
            "Collecting pydantic\n",
            "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (2.32.3)\n",
            "Collecting ruff\n",
            "  Downloading ruff-0.6.9-py3-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.5.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (13 kB)\n",
            "Collecting torch\n",
            "  Downloading torch-2.4.1-cp311-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.0 kB)\n",
            "Collecting tqdm\n",
            "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting transformers==4.45\n",
            "  Downloading transformers-4.45.0-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting types-requests\n",
            "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting types-tqdm\n",
            "  Downloading types_tqdm-4.66.0.20240417-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting filelock (from transformers==4.45)\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45)\n",
            "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers==4.45) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers==4.45) (6.0.2)\n",
            "Collecting regex!=2019.12.17 (from transformers==4.45)\n",
            "  Downloading regex-2024.9.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45)\n",
            "  Downloading tokenizers-0.20.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers==4.45)\n",
            "  Downloading safetensors-0.4.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets)\n",
            "  Downloading pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.10.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.11/site-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: decorator in ./.venv/lib/python3.11/site-packages (from ipython) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.11/site-packages (from ipython) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.11/site-packages (from ipython) (0.1.7)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.11/site-packages (from ipython) (3.0.48)\n",
            "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.11/site-packages (from ipython) (2.18.0)\n",
            "Requirement already satisfied: stack-data in ./.venv/lib/python3.11/site-packages (from ipython) (0.6.3)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in ./.venv/lib/python3.11/site-packages (from ipython) (5.14.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in ./.venv/lib/python3.11/site-packages (from ipython) (4.12.2)\n",
            "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.11/site-packages (from ipython) (4.9.0)\n",
            "Requirement already satisfied: notebook in ./.venv/lib/python3.11/site-packages (from jupyter) (7.2.2)\n",
            "Requirement already satisfied: jupyter-console in ./.venv/lib/python3.11/site-packages (from jupyter) (6.6.3)\n",
            "Requirement already satisfied: nbconvert in ./.venv/lib/python3.11/site-packages (from jupyter) (7.16.4)\n",
            "Requirement already satisfied: ipykernel in ./.venv/lib/python3.11/site-packages (from jupyter) (6.29.5)\n",
            "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.11/site-packages (from jupyter) (8.1.5)\n",
            "Requirement already satisfied: jupyterlab in ./.venv/lib/python3.11/site-packages (from jupyter) (4.2.5)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.54.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (163 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.7-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Collecting joblib (from nltk)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai) (4.6.0)\n",
            "Collecting distro<2,>=1.7.0 (from openai)\n",
            "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from openai) (0.27.2)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.6.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.23.4 (from pydantic)\n",
            "  Downloading pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn)\n",
            "  Downloading scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting sympy (from torch)\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch)\n",
            "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.4)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
            "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
            "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.14.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (52 kB)\n",
            "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython) (0.8.4)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in ./.venv/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: appnope in ./.venv/lib/python3.11/site-packages (from ipykernel->jupyter) (0.1.4)\n",
            "Requirement already satisfied: comm>=0.1.1 in ./.venv/lib/python3.11/site-packages (from ipykernel->jupyter) (0.2.2)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in ./.venv/lib/python3.11/site-packages (from ipykernel->jupyter) (1.8.6)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in ./.venv/lib/python3.11/site-packages (from ipykernel->jupyter) (8.6.3)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.venv/lib/python3.11/site-packages (from ipykernel->jupyter) (5.7.2)\n",
            "Requirement already satisfied: nest-asyncio in ./.venv/lib/python3.11/site-packages (from ipykernel->jupyter) (1.6.0)\n",
            "Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from ipykernel->jupyter) (6.0.0)\n",
            "Requirement already satisfied: pyzmq>=24 in ./.venv/lib/python3.11/site-packages (from ipykernel->jupyter) (26.2.0)\n",
            "Requirement already satisfied: tornado>=6.1 in ./.venv/lib/python3.11/site-packages (from ipykernel->jupyter) (6.4.1)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.12 in ./.venv/lib/python3.11/site-packages (from ipywidgets->jupyter) (4.0.13)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in ./.venv/lib/python3.11/site-packages (from ipywidgets->jupyter) (3.0.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.0)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in ./.venv/lib/python3.11/site-packages (from jupyterlab->jupyter) (2.0.4)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in ./.venv/lib/python3.11/site-packages (from jupyterlab->jupyter) (2.2.5)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in ./.venv/lib/python3.11/site-packages (from jupyterlab->jupyter) (2.14.2)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in ./.venv/lib/python3.11/site-packages (from jupyterlab->jupyter) (2.27.3)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in ./.venv/lib/python3.11/site-packages (from jupyterlab->jupyter) (0.2.4)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: bleach!=5.0.0 in ./.venv/lib/python3.11/site-packages (from nbconvert->jupyter) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in ./.venv/lib/python3.11/site-packages (from nbconvert->jupyter) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in ./.venv/lib/python3.11/site-packages (from nbconvert->jupyter) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in ./.venv/lib/python3.11/site-packages (from nbconvert->jupyter) (3.0.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in ./.venv/lib/python3.11/site-packages (from nbconvert->jupyter) (0.10.0)\n",
            "Requirement already satisfied: nbformat>=5.7 in ./.venv/lib/python3.11/site-packages (from nbconvert->jupyter) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in ./.venv/lib/python3.11/site-packages (from nbconvert->jupyter) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in ./.venv/lib/python3.11/site-packages (from nbconvert->jupyter) (1.3.0)\n",
            "Collecting pytz>=2020.1 (from pandas->datasets)\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
            "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
            "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.11/site-packages (from stack-data->ipython) (2.1.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.11/site-packages (from stack-data->ipython) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in ./.venv/lib/python3.11/site-packages (from stack-data->ipython) (0.2.3)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: webencodings in ./.venv/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in ./.venv/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.3.6)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in ./.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (23.1.0)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in ./.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.10.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in ./.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in ./.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in ./.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.21.0)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in ./.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in ./.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in ./.venv/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
            "Requirement already satisfied: babel>=2.10 in ./.venv/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.16.0)\n",
            "Requirement already satisfied: json5>=0.9.0 in ./.venv/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.9.25)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in ./.venv/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.23.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in ./.venv/lib/python3.11/site-packages (from nbformat>=5.7->nbconvert->jupyter) (2.20.0)\n",
            "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets)\n",
            "  Downloading propcache-0.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: argon2-cffi-bindings in ./.venv/lib/python3.11/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (21.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.20.0)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in ./.venv/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.0.7)\n",
            "Requirement already satisfied: rfc3339-validator in ./.venv/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in ./.venv/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
            "Requirement already satisfied: fqdn in ./.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.5.1)\n",
            "Requirement already satisfied: isoduration in ./.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in ./.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.0.0)\n",
            "Requirement already satisfied: uri-template in ./.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in ./.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (24.8.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in ./.venv/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.17.1)\n",
            "Requirement already satisfied: pycparser in ./.venv/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.22)\n",
            "Requirement already satisfied: arrow>=0.15.0 in ./.venv/lib/python3.11/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in ./.venv/lib/python3.11/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.9.0.20241003)\n",
            "Downloading transformers-4.45.0-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
            "Downloading matplotlib-3.9.2-cp311-cp311-macosx_11_0_arm64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.2-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.51.2-py3-none-any.whl (383 kB)\n",
            "Downloading pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading polars-1.9.0-cp38-abi3-macosx_11_0_arm64.whl (28.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.2/28.2 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
            "Downloading pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.6.9-py3-none-macosx_11_0_arm64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.2-cp311-cp311-macosx_12_0_arm64.whl (11.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.1-cp311-none-macosx_11_0_arm64.whl (62.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.4.2-py3-none-any.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.2/869.2 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.19.1-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
            "Downloading types_requests-2.32.0.20240914-py3-none-any.whl (15 kB)\n",
            "Downloading types_tqdm-4.66.0.20240417-py3-none-any.whl (19 kB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading contourpy-1.3.0-cp311-cp311-macosx_11_0_arm64.whl (250 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading fonttools-4.54.1-cp311-cp311-macosx_11_0_arm64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
            "Downloading aiohttp-3.10.9-cp311-cp311-macosx_11_0_arm64.whl (391 kB)\n",
            "Downloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
            "Downloading jiter-0.6.1-cp311-cp311-macosx_11_0_arm64.whl (302 kB)\n",
            "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Downloading kiwisolver-1.4.7-cp311-cp311-macosx_11_0_arm64.whl (64 kB)\n",
            "Downloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
            "Downloading pyarrow-17.0.0-cp311-cp311-macosx_11_0_arm64.whl (27.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
            "Downloading regex-2024.9.11-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
            "Downloading safetensors-0.4.5-cp311-cp311-macosx_11_0_arm64.whl (381 kB)\n",
            "Downloading scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl (23.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Downloading tokenizers-0.20.0-cp311-cp311-macosx_11_0_arm64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
            "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl (53 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Downloading multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl (29 kB)\n",
            "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
            "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
            "Downloading yarl-1.14.0-cp311-cp311-macosx_11_0_arm64.whl (85 kB)\n",
            "Downloading propcache-0.2.0-cp311-cp311-macosx_11_0_arm64.whl (45 kB)\n",
            "Installing collected packages: pytz, mpmath, xxhash, tzdata, types-tqdm, types-requests, tqdm, threadpoolctl, sympy, safetensors, ruff, regex, PySocks, pyparsing, pydantic-core, propcache, polars, pillow, numpy, networkx, multidict, lightning-utilities, kiwisolver, joblib, jiter, fsspec, frozenlist, fonttools, filelock, distro, dill, cycler, click, annotated-types, aiohappyeyeballs, yarl, torch, scipy, pydantic, pyarrow, pandas, nltk, multiprocess, huggingface-hub, contourpy, aiosignal, torchvision, torchmetrics, tokenizers, scikit-learn, openai, matplotlib, gdown, aiohttp, transformers, datasets\n",
            "Successfully installed PySocks-1.7.1 aiohappyeyeballs-2.4.3 aiohttp-3.10.9 aiosignal-1.3.1 annotated-types-0.7.0 click-8.1.7 contourpy-1.3.0 cycler-0.12.1 datasets-3.0.1 dill-0.3.8 distro-1.9.0 filelock-3.16.1 fonttools-4.54.1 frozenlist-1.4.1 fsspec-2024.6.1 gdown-5.2.0 huggingface-hub-0.25.1 jiter-0.6.1 joblib-1.4.2 kiwisolver-1.4.7 lightning-utilities-0.11.7 matplotlib-3.9.2 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 networkx-3.3 nltk-3.9.1 numpy-2.1.2 openai-1.51.2 pandas-2.2.3 pillow-10.4.0 polars-1.9.0 propcache-0.2.0 pyarrow-17.0.0 pydantic-2.9.2 pydantic-core-2.23.4 pyparsing-3.1.4 pytz-2024.2 regex-2024.9.11 ruff-0.6.9 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sympy-1.13.3 threadpoolctl-3.5.0 tokenizers-0.20.0 torch-2.4.1 torchmetrics-1.4.2 torchvision-0.19.1 tqdm-4.66.5 transformers-4.45.0 types-requests-2.32.0.20240914 types-tqdm-4.66.0.20240417 tzdata-2024.2 xxhash-3.5.0 yarl-1.14.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install all packages listed in pyproject.toml\n",
        "%pip install click datasets gdown ipython jupyter matplotlib nltk numpy openai pillow polars pydantic requests ruff scikit-learn torch torchmetrics torchvision tqdm transformers==4.45 types-requests types-tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zZVpUWUluZbV"
      },
      "source": [
        "Select \"GPU\" in the Accelerator drop-down on Notebook Settings through the Edit menu.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "8XYfp8LNcRD3"
      },
      "outputs": [],
      "source": [
        "# %pip install torch torchvision numpy matplotlib Pillow datasets\n",
        "import torch\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nkd4MHFQv0jS"
      },
      "source": [
        "## Confirm PyTorch can see the GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nbotnVwUpWBa"
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IRPpWCXA05ka"
      },
      "source": [
        "## Additional information about hardware\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k8XAtu4u1mXZ"
      },
      "source": [
        "For CPU information and RAM, run:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4Mr3-8s-1jPB"
      },
      "outputs": [],
      "source": [
        "!cat /proc/cpuinfo\n",
        "!cat /proc/meminfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ic-CaNISucO-"
      },
      "source": [
        "## Other useful package imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gevJulhruagf"
      },
      "outputs": [],
      "source": [
        "import importlib\n",
        "import operator\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from functools import reduce\n",
        "from os import path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0kDf1Kmntpwo"
      },
      "source": [
        "# Working on the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pQe3wu4U5kOt"
      },
      "source": [
        "The dataset is a subset of the [RVL-CDIP dataset](https://www.cs.cmu.edu/~aharley/rvl-cdip/). See [Harley et al.](http://scs.ryerson.ca/~aharley/icdar15/harley_convnet_icdar15.pdf) and [Asim et al.](https://www.dfki.de/fileadmin/user_upload/import/10637_Asim_Document_Image_Classification.pdf) papers for recent works on this dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jH2SxwR_1Rpu"
      },
      "source": [
        "## Information about the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lxvfM7Wn_YUQ"
      },
      "source": [
        "This project only considers the following 5 classes among the 16 classes of the original dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "lGLInZca1Pbg"
      },
      "outputs": [],
      "source": [
        "class_names = [\"email\", \"form\", \"handwritten\", \"invoice\", \"advertisement\"]\n",
        "NUM_CLASSES = len(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XgN4fpA0uO8n"
      },
      "source": [
        "## Import the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AVnJr5tzswrq"
      },
      "source": [
        "If you are on Google Colab, first clone the repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ST3fUpSmqncY"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"NeuralDocumentClassification\"):\n",
        "    !git clone https://github.com/thibaultdouzon/NeuralDocumentClassification.git\n",
        "else:\n",
        "    !git -C NeuralDocumentClassification pull\n",
        "sys.path.append(\"NeuralDocumentClassification\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9zWbX_GYypxH"
      },
      "source": [
        "You now either have a \"NeuralDocumentClassification\" folder or are already inside it.\n",
        "Download the train, test and validation dataset assignments from this [Google Drive](https://drive.google.com/drive/folders/1Pkd6sUkDGBUymWKK93abZx1MQiWmzFgP) using the provided code in `src.download_dataset`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pQJ8Kqy3sv_v"
      },
      "outputs": [],
      "source": [
        "from src import download_dataset\n",
        "\n",
        "dataset_path = \"dataset\"\n",
        "\n",
        "download_dataset.download_and_extract(\"all\", dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VYoAg4lOau3h"
      },
      "source": [
        "Each dataset file is a binary dump that can be loaded with the [Pickle](https://docs.python.org/3.11/library/pickle.html) module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "a4G-8jVJauWC"
      },
      "outputs": [],
      "source": [
        "with open(path.join(dataset_path, \"train.pkl\"), \"rb\") as f:\n",
        "    train_dataset = pickle.load(f)\n",
        "\n",
        "with open(path.join(dataset_path, \"test.pkl\"), \"rb\") as f:\n",
        "    test_dataset = pickle.load(f)\n",
        "\n",
        "with open(path.join(dataset_path, \"validation.pkl\"), \"rb\") as f:\n",
        "    validation_dataset = pickle.load(f)\n",
        "\n",
        "\n",
        "for split_name, split_dataset in zip(\n",
        "    [\"train\", \"test\", \"validation\"], [train_dataset, test_dataset, validation_dataset]\n",
        "):\n",
        "    print(f\"{split_name}_dataset contains {len(split_dataset)} documents\")\n",
        "train_dataset[0].keys()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each `dataset` object is a `list` containing multiple document information. A document is a `dict` with the following structure:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"id\": \"Unique document identifier\",\n",
        "  \"image\": \"A PIL.Image object containing the document's image\",\n",
        "  \"label\": \"A number between in [0 .. 4] representing the class of the document\",\n",
        "  \"words\": \"A list of words extracted from the image with an OCR\",\n",
        "  \"boxes\": \"A list of tuples of numbers providing the position of each word in the document\"\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "93EnbGGRid77"
      },
      "source": [
        "Print 5 image from the training dataset using [matplotlib](https://matplotlib.org/stable/tutorials/images.html)'s `plt` module:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Lw4UmzI2_FV_"
      },
      "outputs": [],
      "source": [
        "### Insert your code here ###\n",
        "# See the expected solution by clicking on the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "xlg7AAuoiAPU"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "for document in train_dataset[:5]:\n",
        "    print(class_names[document[\"label\"]])\n",
        "    plt.imshow(document[\"image\"].convert(\"RGB\"))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try to answer the following questions:\n",
        "\n",
        "What is the shape of the images?\n",
        "How are the different classes distributed?\n",
        "Using subplots, show an image of each class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pTo64W_682WR"
      },
      "source": [
        "# Creating Pytorch datasets and dataloaders for Computer Vision task\n",
        "\n",
        "The first goal of this section is to create `torch.utils.data.Dataset` for the classification task using only the image of the document.\n",
        "\n",
        "We will define a class inheriting [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) called `DocumentImageDataset`.\n",
        "\n",
        "It should be able to create an instance of `DocumentImageDataset` using our previously loaded datasets.\n",
        "For simplification, all images should be resized to a fixed (512, 512) size. Use [`torchvision.transforms.v2.functional`](https://pytorch.org/vision/main/transforms.html#v2-api-reference-recommended) module to convert a `PIL.Image` to a `torch.Tensor` and perform the simplifications.\n",
        "\n",
        "Upon iteration, it should return an `ImageSample` object defined as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms.v2.functional as F\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ImageSample:\n",
        "    image: torch.Tensor  # shape: (C, H, W)\n",
        "    label: int  # 0 ≤ label < NUM_CLASSES\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"Some assertions to check the validity of the data\"\n",
        "        assert self.image.shape == (\n",
        "            1,\n",
        "            512,\n",
        "            512,\n",
        "        ), f\"Expected shape (1, 512, 512), got {self.image.shape}\"\n",
        "        assert torch.all(self.image <= 1.0) and torch.all(\n",
        "            self.image >= 0.0\n",
        "        ), \"Expected each pixel of image in range [0.0, 1.0]\"\n",
        "        assert self.label in range(\n",
        "            NUM_CLASSES\n",
        "        ), f\"Expected label in range [0 .. {NUM_CLASSES-1}], got {self.label}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "e7QYsgFr88jO"
      },
      "outputs": [],
      "source": [
        "# Fill the methods of the class DocumentImageDataset\n",
        "\n",
        "\n",
        "class DocumentImageDataset(data.Dataset):\n",
        "    def __init__(self, dataset: list[dict]):\n",
        "        self.dataset = dataset\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"This method returns the length of the dataset\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __getitem__(self, idx: int) -> ImageSample:\n",
        "        \"\"\"This method returns the idx-th sample of the dataset\n",
        "        If idx is out of bounds, it should raise an IndexError\"\"\"\n",
        "\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "class DocumentImageDataset(data.Dataset):\n",
        "    def __init__(self, dataset: list[dict]):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"This method returns the length of the dataset\"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> ImageSample:\n",
        "        \"\"\"This method returns the idx-th sample of the dataset\n",
        "        If idx is out of bounds, it should raise an IndexError\"\"\"\n",
        "\n",
        "        return ImageSample(\n",
        "            # F.to_tensor is deprecated, use F.to_dtype(F.to_image(...), dtype=torch.float32, scale=True) instead\n",
        "            image=F.to_dtype(\n",
        "                F.to_image(F.resize(self.dataset[idx][\"image\"], size=[512, 512])),\n",
        "                dtype=torch.float32,\n",
        "                scale=True,\n",
        "            ),\n",
        "            label=self.dataset[idx][\"label\"],\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your implementation is correct, you should be able to create an instance of `DocumentImageDataset` and get its 0th element without error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_dataset = DocumentImageDataset(validation_dataset)\n",
        "image_dataset[0]  # no error here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The final goal of this section is to implement a [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) that wraps the `DocumentImageDataset` and handles useful tasks like shuffling and batching.\n",
        "\n",
        "No need to create a new class, we simply need to implement the `collate_fn` that takes a list of `ImageSample` and should return an `ImageBatch`.\n",
        "\n",
        "hint: Use `torch.tensor` and `torch.stack` to respectively convert a python list to a `torch.Tensor` and stack multiple tensors together into a new one along a new dimension.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ImageBatch:\n",
        "    images: torch.Tensor\n",
        "    labels: torch.Tensor\n",
        "\n",
        "    def __post_init__(self):\n",
        "        assert self.images.shape[0] == self.labels.shape[0]\n",
        "        assert self.images.shape[1:] == (1, 512, 512)\n",
        "        assert len(self.images.shape) == 4\n",
        "        assert len(self.labels.shape) == 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch: list[ImageSample]) -> ImageBatch:\n",
        "    \"\"\"This function should return a batch of samples as an ImageBatch object\"\"\"\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "def collate_fn(batch: list[ImageSample]) -> ImageBatch:\n",
        "    \"\"\"This function should return a batch of samples as an ImageBatch object\"\"\"\n",
        "    return ImageBatch(\n",
        "        images=torch.stack(\n",
        "            [sample.image for sample in batch], dim=0\n",
        "        ),  # shape: (B, C, H, W)\n",
        "        labels=torch.tensor([sample.label for sample in batch]),  # shape: (B,)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your implementation is correct, you should be able to create a dataloader with a batch size and retrieve the first batch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataloader = data.DataLoader(\n",
        "    image_dataset, batch_size=5, collate_fn=collate_fn, shuffle=True, drop_last=True\n",
        ")\n",
        "next(iter(dataloader))  # no error here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9oQZqkwA5mRU"
      },
      "source": [
        "# Visual classifiers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JcRvGBPJCPAI"
      },
      "source": [
        "## Multi Layer Perceptron\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hmD69yYP8Z7X"
      },
      "source": [
        "### Set up the layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ooKloCG47hA4"
      },
      "source": [
        "Build a neural network composed of one fully connected (aka dense, or `Linear` in torch) hidden layer with 128 [ReLu](<https://en.wikipedia.org/wiki/Rectifier_(neural_networks)>) units.\n",
        "\n",
        "Each image must be flattened to a single (512 × 512) dimension before being fed to the linear layer.\n",
        "\n",
        "Use `torch.nn` (nn stands for Neural Network) module for all those operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YCIzEefKKKNK"
      },
      "outputs": [],
      "source": [
        "mlp_model = nn.Sequential(\n",
        "    # Fill the layers of the model\n",
        "    # It should take an input of shape (B, 512, 512)\n",
        "    # and output a tensor of shape (B, NUM_CLASSES)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "HYxVCSLQ5tmk"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "mlp_model = nn.Sequential(\n",
        "    nn.Flatten(start_dim=1),  # Do not flatten the batch dimension\n",
        "    nn.Linear(512 * 512, 128),  # d_input = n_pixels in an image = 512 × 512\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, NUM_CLASSES),  # d_output = NUM_CLASSES\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Side question, how many trainable parameters does your model have ?\n",
        "\n",
        "hint: use the `model.parameters()` method to iterate over all the model's parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "def count_parameters(model, trainable=True):\n",
        "    return sum(\n",
        "        reduce(operator.mul, p.shape, 1)  # or p.numel()\n",
        "        for p in model.parameters()\n",
        "        if p.requires_grad == trainable\n",
        "    )\n",
        "\n",
        "\n",
        "print(f\"Your model uses {count_parameters(mlp_model):_} trainable parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-GFuLNGh9U5w"
      },
      "source": [
        "### Train the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pytorch does not provide a ready to use training loop function like Tensorflow does.\n",
        "We will implement it ourselves.\n",
        "\n",
        "We must first implement the training over a full iteration over the dataloader.\n",
        "It will take the model, the dataloader, a loss function, an optimizer and a device to run on.\n",
        "\n",
        "hint: help yourselves with the torch [documentation](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vO1F5yxIMgY-"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    device: torch.device,\n",
        ") -> float:\n",
        "    \"\"\"This function should train the model for one epoch and return the average loss\"\"\"\n",
        "\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    device: torch.device,\n",
        ") -> float:\n",
        "    \"\"\"This function should train the model for one epoch and return the average loss\"\"\"\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    with tqdm.tqdm(desc=\"Training\", total=len(dataloader)) as pbar:\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            images, labels = batch.images.to(device), batch.labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "            outputs = model(images)  # Compute model's predictions\n",
        "            loss = loss_fn(outputs, labels)  # Compute the loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            pbar.set_postfix(loss=epoch_loss / (i + 1))\n",
        "            pbar.update(1)\n",
        "    mean_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Training loss (↓): {mean_loss:.4f}\")\n",
        "    return mean_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need to implement an evaluation method that evaluates the model's performance on a test or validation set.\n",
        "\n",
        "It might compute the average loss and performance metric that we will use to compare models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    metric_fn: nn.Module,\n",
        "    device: torch.device,\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"This function should evaluate the model on the dataset and return the average loss and metric\"\"\"\n",
        "\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    metric_fn: nn.Module,\n",
        "    device: torch.device,\n",
        "    dataset_name: str = \"validation\",\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"This function should evaluate the model on the dataset and return the average loss and metric\"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    epoch_metric = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(dataloader, desc=\"Evaluation\"):\n",
        "            images, labels = batch.images.to(device), batch.labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            metric = metric_fn(outputs.argmax(dim=-1), labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_metric += metric.item()\n",
        "\n",
        "        mean_loss = epoch_loss / len(dataloader)\n",
        "        print(f\"{dataset_name.capitalize()} loss (↓): {mean_loss:.4f}\")\n",
        "        mean_metric = epoch_metric / len(dataloader)\n",
        "        print(f\"{dataset_name.capitalize()} metric (↑): {mean_metric:.4f}\")\n",
        "        return mean_loss, mean_metric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now implement the outer loop that trains the model over several epochs.\n",
        "\n",
        "After each epoch, we want to control the model's performance on the validation set.\n",
        "\n",
        "More confisticated training procedures might include model savings, modifying the learning rate or reporting to a dashboard.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_dataloader: data.DataLoader,\n",
        "    validation_dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    metric_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    device: torch.device,\n",
        "    n_epochs: int = 10,\n",
        ") -> tuple[list[float], list[float], list[float]]:\n",
        "    \"\"\"This function should train the model for 10 epochs and return the training and validation losses and metrics\"\"\"\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
        "        # Train the model here\n",
        "\n",
        "        # Evaluate the model here\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_dataloader: data.DataLoader,\n",
        "    validation_dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    metric_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    device: torch.device,\n",
        "    n_epochs: int = 10,\n",
        ") -> tuple[list[float], list[float], list[float]]:\n",
        "    \"\"\"This function should train the model for some epochs and return the training and validation losses\"\"\"\n",
        "    train_losses = []\n",
        "    validation_losses = []\n",
        "    validation_metrics = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
        "        train_loss = train_one_epoch(\n",
        "            model, train_dataloader, loss_fn, optimizer, device\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        validation_loss, validation_metric = evaluate(\n",
        "            model, validation_dataloader, loss_fn, metric_fn, device\n",
        "        )\n",
        "        validation_losses.append(validation_loss)\n",
        "        validation_metrics.append(validation_metric)\n",
        "\n",
        "    return train_losses, validation_losses, validation_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's put it all together\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "    DocumentImageDataset(train_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        ")\n",
        "validation_loader = data.DataLoader(\n",
        "    DocumentImageDataset(validation_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "device = torch.device(\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "selected_model = mlp_model\n",
        "\n",
        "optimizer = torch.optim.Adam(selected_model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "metric_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "n_epochs = 5\n",
        "\n",
        "hist = train(\n",
        "    selected_model,\n",
        "    train_loader,\n",
        "    validation_loader,\n",
        "    loss_fn,\n",
        "    metric_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    n_epochs=n_epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the losses and the accuracies on 2 different subplots to observe how the training went.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# First subplot\n",
        "plt.subplot(1, 2, 1)\n",
        "# Subplot code here\n",
        "\n",
        "# Second subplot\n",
        "plt.subplot(1, 2, 2)\n",
        "# Subplot code here\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.plot(hist[0], label=\"Training loss\")\n",
        "plt.plot(hist[1], label=\"Validation loss\")\n",
        "plt.legend()\n",
        "plt.yscale(\"log\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(hist[2], label=\"Validation accuracy\")\n",
        "plt.legend()\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.ylim(0, 1)\n",
        "plt.xlabel(\"Epoch\")\n",
        "\n",
        "# figure title\n",
        "plt.suptitle(\"Training history\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nfLk6pw2M08a"
      },
      "source": [
        "### Evaluation on the test set\n",
        "\n",
        "Now evaluate the model on the remaining test set and store its accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "wq93ddH0NDpS"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "test_loader = data.DataLoader(\n",
        "    DocumentImageDataset(test_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "test_loss, test_metric = evaluate(\n",
        "    mlp_model, test_loader, loss_fn, metric_fn, device, dataset_name=\"test\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x_gKCPtWVg3M"
      },
      "source": [
        "Are these values different from their training counterparts ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LnMkYilGngXs"
      },
      "source": [
        "## Convolutional Neural Networks (CNN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kaz6G-cbotNS"
      },
      "source": [
        "### Training from scratch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gNifb_-ImoL1"
      },
      "source": [
        "Create and compile a model alterning convolution and max pooling layers. You can add some fully connected layers between the last locally connected layer and the output layer. Start with a shallow network (4 or 5 convolution layers) and progressively move to deeper architectures:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5VSo8vjiGOUc"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "conv_model = nn.Sequential(\n",
        "    nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Flatten(start_dim=1),\n",
        "    nn.Linear(128 * 128 * 16, NUM_CLASSES),\n",
        ")\n",
        "\n",
        "print(f\"Your model uses {count_parameters(conv_model):_} trainable parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the CNN model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "R-fLDDzLmjMH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "import torchmetrics\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "    DocumentImageDataset(train_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        ")\n",
        "validation_loader = data.DataLoader(\n",
        "    DocumentImageDataset(validation_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "device = torch.device(\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "selected_model = conv_model\n",
        "\n",
        "optimizer = torch.optim.Adam(selected_model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "metric_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "n_epochs = 5\n",
        "\n",
        "hist = train(\n",
        "    selected_model,\n",
        "    train_loader,\n",
        "    validation_loader,\n",
        "    loss_fn,\n",
        "    metric_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    n_epochs=n_epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation on the test set\n",
        "\n",
        "How does it compare with the MLP model?\n",
        "What is the best accuracy you can get?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "test_loader = data.DataLoader(\n",
        "    DocumentImageDataset(test_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "test_loss, test_metric = evaluate(\n",
        "    conv_model, test_loader, loss_fn, metric_fn, device, dataset_name=\"test\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using a pre-trained model\n",
        "\n",
        "Download a pre-trained model from the [pytorch hub](https://pytorch.org/vision/stable/models.html#using-the-pre-trained-models) for vision model.\n",
        "\n",
        "Eg. Resnet, EfficientNet, ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "from torchvision.models import list_models\n",
        "\n",
        "print(list_models())\n",
        "\n",
        "pretrained_model = torch.hub.load(\"pytorch/vision\", \"efficientnet_b1\", pretrained=True)\n",
        "pretrained_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, the loaded does not make predictions for our problem.\n",
        "\n",
        "We need to slightly modify its output to fit our requirements.\n",
        "\n",
        "Models trained on ImageNet expect color images with 3 channels for color instead of 1.\n",
        "We either need to modify th first convolution layer of the model to accomodate for that.\n",
        "Or, another solution could be to repeat our input image 3 times along the channel dimension. That could be done in a new `collat_fn`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "# For resnet18\n",
        "# ## convert input to grayscale\n",
        "# pretrained_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "# pretrained_model.conv1.weight = nn.Parameter(pretrained_model.conv1.weight.sum(dim=1, keepdim=True) / 3)\n",
        "\n",
        "# ## new classification head\n",
        "# pretrained_model.fc = nn.Linear(pretrained_model.fc.in_features, NUM_CLASSES)\n",
        "\n",
        "\n",
        "# For efficientnet_b1\n",
        "## convert input to grayscale\n",
        "old_weights = nn.Parameter(\n",
        "    pretrained_model.features[0][0].weight.sum(dim=1, keepdim=True) / 3\n",
        ")\n",
        "pretrained_model.features[0][0] = nn.Conv2d(\n",
        "    1, 32, kernel_size=3, stride=2, padding=1, bias=False\n",
        ")\n",
        "pretrained_model.features[0][0].weight = old_weights\n",
        "\n",
        "pretrained_model.classifier[1] = nn.Linear(\n",
        "    pretrained_model.classifier[1].in_features, NUM_CLASSES\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We advice freezing all parameters except those from the last layer of convolution and the new classification head.\n",
        "\n",
        "It reduces the memory requirements to train the model and ensure the features the pre-trained model was trained to extract are not modified by the finetunig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For efficientnet_b1 only, adapt it to your model\n",
        "\n",
        "for param in pretrained_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "# Train the last layer of convolution group\n",
        "for param in pretrained_model.features[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Train the new classifier group\n",
        "for param in pretrained_model.classifier.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "    DocumentImageDataset(train_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        ")\n",
        "validation_loader = data.DataLoader(\n",
        "    DocumentImageDataset(validation_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "device = torch.device(\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "selected_model = pretrained_model\n",
        "\n",
        "optimizer = torch.optim.Adam(selected_model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "metric_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "n_epochs = 5\n",
        "\n",
        "hist = train(\n",
        "    selected_model,\n",
        "    train_loader,\n",
        "    validation_loader,\n",
        "    loss_fn,\n",
        "    metric_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    n_epochs=n_epochs,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "j5UeZiGlvNoH",
        "n52k6VoU1brz",
        "nkd4MHFQv0jS",
        "IRPpWCXA05ka",
        "ic-CaNISucO-",
        "XgN4fpA0uO8n",
        "_1Eq6TC2wicn",
        "OdGw-l6TEUiP",
        "nfLk6pw2M08a",
        "XXHM8y9kRW5V"
      ],
      "include_colab_link": true,
      "name": "skeleton.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
