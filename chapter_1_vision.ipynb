{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clemsage/NeuralDocumentClassification/blob/master/skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j5UeZiGlvNoH"
      },
      "source": [
        "# Setting up the computing environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n52k6VoU1brz"
      },
      "source": [
        "## Install and import PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install all packages listed in pyproject.toml\n",
        "%pip install click datasets gdown ipython jupyter matplotlib nltk numpy openai pillow polars pydantic requests ruff scikit-learn torch torchmetrics torchvision tqdm transformers types-requests types-tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zZVpUWUluZbV"
      },
      "source": [
        "Select \"GPU\" in the Accelerator drop-down on Notebook Settings through the Edit menu.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "8XYfp8LNcRD3"
      },
      "outputs": [],
      "source": [
        "# %pip install torch torchvision numpy matplotlib Pillow datasets\n",
        "import torch\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nkd4MHFQv0jS"
      },
      "source": [
        "## Confirm PyTorch can see the GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nbotnVwUpWBa"
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IRPpWCXA05ka"
      },
      "source": [
        "## Additional information about hardware\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k8XAtu4u1mXZ"
      },
      "source": [
        "For CPU information and RAM, run:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4Mr3-8s-1jPB"
      },
      "outputs": [],
      "source": [
        "!cat /proc/cpuinfo\n",
        "!cat /proc/meminfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ic-CaNISucO-"
      },
      "source": [
        "## Other useful package imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gevJulhruagf"
      },
      "outputs": [],
      "source": [
        "import importlib\n",
        "import operator\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from functools import reduce\n",
        "from os import path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0kDf1Kmntpwo"
      },
      "source": [
        "# Working on the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pQe3wu4U5kOt"
      },
      "source": [
        "The dataset is a subset of the [RVL-CDIP dataset](https://www.cs.cmu.edu/~aharley/rvl-cdip/). See [Harley et al.](http://scs.ryerson.ca/~aharley/icdar15/harley_convnet_icdar15.pdf) and [Asim et al.](https://www.dfki.de/fileadmin/user_upload/import/10637_Asim_Document_Image_Classification.pdf) papers for recent works on this dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jH2SxwR_1Rpu"
      },
      "source": [
        "## Information about the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lxvfM7Wn_YUQ"
      },
      "source": [
        "This project only considers the following 5 classes among the 16 classes of the original dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "lGLInZca1Pbg"
      },
      "outputs": [],
      "source": [
        "class_names = [\"email\", \"form\", \"handwritten\", \"invoice\", \"advertisement\"]\n",
        "NUM_CLASSES = len(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XgN4fpA0uO8n"
      },
      "source": [
        "## Import the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AVnJr5tzswrq"
      },
      "source": [
        "If you are on Google Colab, first clone the repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ST3fUpSmqncY"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"NeuralDocumentClassification\"):\n",
        "    !git clone https://github.com/thibaultdouzon/NeuralDocumentClassification.git\n",
        "else:\n",
        "    !git -C NeuralDocumentClassification pull\n",
        "sys.path.append(\"NeuralDocumentClassification\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9zWbX_GYypxH"
      },
      "source": [
        "You now either have a \"NeuralDocumentClassification\" folder or are already inside it.\n",
        "Download the train, test and validation dataset assignments from this [Google Drive](https://drive.google.com/drive/folders/1Pkd6sUkDGBUymWKK93abZx1MQiWmzFgP) using the provided code in `src.download_dataset`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pQJ8Kqy3sv_v"
      },
      "outputs": [],
      "source": [
        "from src import download_dataset\n",
        "\n",
        "dataset_path = \"dataset\"\n",
        "\n",
        "download_dataset.download_and_extract(\"all\", dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VYoAg4lOau3h"
      },
      "source": [
        "Each dataset file is a binary dump that can be loaded with the [Pickle](https://docs.python.org/3.11/library/pickle.html) module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "a4G-8jVJauWC"
      },
      "outputs": [],
      "source": [
        "with open(path.join(dataset_path, \"train.pkl\"), \"rb\") as f:\n",
        "    train_dataset = pickle.load(f)\n",
        "\n",
        "with open(path.join(dataset_path, \"test.pkl\"), \"rb\") as f:\n",
        "    test_dataset = pickle.load(f)\n",
        "\n",
        "with open(path.join(dataset_path, \"validation.pkl\"), \"rb\") as f:\n",
        "    validation_dataset = pickle.load(f)\n",
        "\n",
        "\n",
        "for split_name, split_dataset in zip(\n",
        "    [\"train\", \"test\", \"validation\"], [train_dataset, test_dataset, validation_dataset]\n",
        "):\n",
        "    print(f\"{split_name}_dataset contains {len(split_dataset)} documents\")\n",
        "train_dataset[0].keys()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each `dataset` object is a `list` containing multiple document information. A document is a `dict` with the following structure:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"id\": \"Unique document identifier\",\n",
        "  \"image\": \"A PIL.Image object containing the document's image\",\n",
        "  \"label\": \"A number between in [0 .. 4] representing the class of the document\",\n",
        "  \"words\": \"A list of words extracted from the image with an OCR\",\n",
        "  \"boxes\": \"A list of tuples of numbers providing the position of each word in the document\"\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "93EnbGGRid77"
      },
      "source": [
        "Print 5 image from the training dataset using [matplotlib](https://matplotlib.org/stable/tutorials/images.html)'s `plt` module:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Lw4UmzI2_FV_"
      },
      "outputs": [],
      "source": [
        "### Insert your code here ###\n",
        "# See the expected solution by clicking on the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "xlg7AAuoiAPU"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "for document in train_dataset[:5]:\n",
        "    print(class_names[document[\"label\"]])\n",
        "    plt.imshow(document[\"image\"].convert(\"RGB\"))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try to answer the following questions:\n",
        "\n",
        "What is the shape of the images?\n",
        "How are the different classes distributed?\n",
        "Using subplots, show an image of each class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pTo64W_682WR"
      },
      "source": [
        "# Creating Pytorch datasets and dataloaders for Computer Vision task\n",
        "\n",
        "The first goal of this section is to create `torch.utils.data.Dataset` for the classification task using only the image of the document.\n",
        "\n",
        "We will define a class inheriting [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) called `DocumentImageDataset`.\n",
        "\n",
        "It should be able to create an instance of `DocumentImageDataset` using our previously loaded datasets.\n",
        "For simplification, all images should be resized to a fixed (512, 512) size. Use [`torchvision.transforms.v2.functional`](https://pytorch.org/vision/main/transforms.html#v2-api-reference-recommended) module to convert a `PIL.Image` to a `torch.Tensor` and perform the simplifications.\n",
        "\n",
        "Upon iteration, it should return an `ImageSample` object defined as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms.v2.functional as F\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ImageSample:\n",
        "    image: torch.Tensor  # shape: (C, H, W)\n",
        "    label: int  # 0 ≤ label < NUM_CLASSES\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"Some assertions to check the validity of the data\"\n",
        "        assert self.image.shape == (\n",
        "            1,\n",
        "            512,\n",
        "            512,\n",
        "        ), f\"Expected shape (1, 512, 512), got {self.image.shape}\"\n",
        "        assert torch.all(self.image <= 1.0) and torch.all(\n",
        "            self.image >= 0.0\n",
        "        ), \"Expected each pixel of image in range [0.0, 1.0]\"\n",
        "        assert self.label in range(\n",
        "            NUM_CLASSES\n",
        "        ), f\"Expected label in range [0 .. {NUM_CLASSES-1}], got {self.label}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "e7QYsgFr88jO"
      },
      "outputs": [],
      "source": [
        "# Fill the methods of the class DocumentImageDataset\n",
        "\n",
        "\n",
        "class DocumentImageDataset(data.Dataset):\n",
        "    def __init__(self, dataset: list[dict]):\n",
        "        self.dataset = dataset\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"This method returns the length of the dataset\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __getitem__(self, idx: int) -> ImageSample:\n",
        "        \"\"\"This method returns the idx-th sample of the dataset\n",
        "        If idx is out of bounds, it should raise an IndexError\"\"\"\n",
        "\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "class DocumentImageDataset(data.Dataset):\n",
        "    def __init__(self, dataset: list[dict]):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"This method returns the length of the dataset\"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> ImageSample:\n",
        "        \"\"\"This method returns the idx-th sample of the dataset\n",
        "        If idx is out of bounds, it should raise an IndexError\"\"\"\n",
        "\n",
        "        return ImageSample(\n",
        "            # F.to_tensor is deprecated, use F.to_dtype(F.to_image(...), dtype=torch.float32, scale=True) instead\n",
        "            image=F.to_dtype(\n",
        "                F.to_image(F.resize(self.dataset[idx][\"image\"], size=[512, 512])),\n",
        "                dtype=torch.float32,\n",
        "                scale=True,\n",
        "            ),\n",
        "            label=self.dataset[idx][\"label\"],\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your implementation is correct, you should be able to create an instance of `DocumentImageDataset` and get its 0th element without error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_dataset = DocumentImageDataset(validation_dataset)\n",
        "image_dataset[0]  # no error here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The final goal of this section is to implement a [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) that wraps the `DocumentImageDataset` and handles useful tasks like shuffling and batching.\n",
        "\n",
        "No need to create a new class, we simply need to implement the `collate_fn` that takes a list of `ImageSample` and should return an `ImageBatch`.\n",
        "\n",
        "hint: Use `torch.tensor` and `torch.stack` to respectively convert a python list to a `torch.Tensor` and stack multiple tensors together into a new one along a new dimension.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ImageBatch:\n",
        "    images: torch.Tensor\n",
        "    labels: torch.Tensor\n",
        "\n",
        "    def __post_init__(self):\n",
        "        assert self.images.shape[0] == self.labels.shape[0]\n",
        "        assert self.images.shape[1:] == (1, 512, 512)\n",
        "        assert len(self.images.shape) == 4\n",
        "        assert len(self.labels.shape) == 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch: list[ImageSample]) -> ImageBatch:\n",
        "    \"\"\"This function should return a batch of samples as an ImageBatch object\"\"\"\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "def collate_fn(batch: list[ImageSample]) -> ImageBatch:\n",
        "    \"\"\"This function should return a batch of samples as an ImageBatch object\"\"\"\n",
        "    return ImageBatch(\n",
        "        images=torch.stack(\n",
        "            [sample.image for sample in batch], dim=0\n",
        "        ),  # shape: (B, C, H, W)\n",
        "        labels=torch.tensor([sample.label for sample in batch]),  # shape: (B,)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your implementation is correct, you should be able to create a dataloader with a batch size and retrieve the first batch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataloader = data.DataLoader(\n",
        "    image_dataset, batch_size=5, collate_fn=collate_fn, shuffle=True, drop_last=True\n",
        ")\n",
        "next(iter(dataloader))  # no error here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9oQZqkwA5mRU"
      },
      "source": [
        "# Visual classifiers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JcRvGBPJCPAI"
      },
      "source": [
        "## Multi Layer Perceptron\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hmD69yYP8Z7X"
      },
      "source": [
        "### Set up the layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ooKloCG47hA4"
      },
      "source": [
        "Build a neural network composed of one fully connected (aka dense, or `Linear` in torch) hidden layer with 128 [ReLu](<https://en.wikipedia.org/wiki/Rectifier_(neural_networks)>) units.\n",
        "\n",
        "Each image must be flattened to a single (512 × 512) dimension before being fed to the linear layer.\n",
        "\n",
        "Use `torch.nn` (nn stands for Neural Network) module for all those operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YCIzEefKKKNK"
      },
      "outputs": [],
      "source": [
        "mlp_model = nn.Sequential(\n",
        "    # Fill the layers of the model\n",
        "    # It should take an input of shape (B, 512, 512)\n",
        "    # and output a tensor of shape (B, NUM_CLASSES)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "HYxVCSLQ5tmk"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "mlp_model = nn.Sequential(\n",
        "    nn.Flatten(start_dim=1),  # Do not flatten the batch dimension\n",
        "    nn.Linear(512 * 512, 128),  # d_input = n_pixels in an image = 512 × 512\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, NUM_CLASSES),  # d_output = NUM_CLASSES\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Side question, how many trainable parameters does your model have ?\n",
        "\n",
        "hint: use the `model.parameters()` method to iterate over all the model's parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_parameters(model, trainable=True):\n",
        "    return sum(\n",
        "        reduce(operator.mul(p.shape), 1)  # or p.numel()\n",
        "        for p in model.parameters()\n",
        "        if p.requires_grad == trainable\n",
        "    )\n",
        "\n",
        "\n",
        "print(f\"Your model uses {count_parameters(mlp_model):_} trainable parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-GFuLNGh9U5w"
      },
      "source": [
        "### Train the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pytorch does not provide a ready to use training loop function like Tensorflow does.\n",
        "We will implement it ourselves.\n",
        "\n",
        "We must first implement the training over a full iteration over the dataloader.\n",
        "It will take the model, the dataloader, a loss function, an optimizer and a device to run on.\n",
        "\n",
        "hint: help yourselves with the torch [documentation](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vO1F5yxIMgY-"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    device: torch.device,\n",
        ") -> float:\n",
        "    \"\"\"This function should train the model for one epoch and return the average loss\"\"\"\n",
        "\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    device: torch.device,\n",
        ") -> float:\n",
        "    \"\"\"This function should train the model for one epoch and return the average loss\"\"\"\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    with tqdm.tqdm(desc=\"Training\", total=len(dataloader)) as pbar:\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            images, labels = batch.images.to(device), batch.labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "            outputs = model(images)  # Compute model's predictions\n",
        "            loss = loss_fn(outputs, labels)  # Compute the loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            pbar.set_postfix(loss=epoch_loss / (i + 1))\n",
        "            pbar.update(1)\n",
        "    mean_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Training loss (↓): {mean_loss:.4f}\")\n",
        "    return mean_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need to implement an evaluation method that evaluates the model's performance on a test or validation set.\n",
        "\n",
        "It might compute the average loss and performance metric that we will use to compare models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    metric_fn: nn.Module,\n",
        "    device: torch.device,\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"This function should evaluate the model on the dataset and return the average loss and metric\"\"\"\n",
        "\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    metric_fn: nn.Module,\n",
        "    device: torch.device,\n",
        "    dataset_name: str = \"validation\",\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"This function should evaluate the model on the dataset and return the average loss and metric\"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    epoch_metric = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(dataloader, desc=\"Evaluation\"):\n",
        "            images, labels = batch.images.to(device), batch.labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            metric = metric_fn(outputs.argmax(dim=-1), labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_metric += metric.item()\n",
        "\n",
        "        mean_loss = epoch_loss / len(dataloader)\n",
        "        print(f\"{dataset_name.capitalize()} loss (↓): {mean_loss:.4f}\")\n",
        "        mean_metric = epoch_metric / len(dataloader)\n",
        "        print(f\"{dataset_name.capitalize()} metric (↑): {mean_metric:.4f}\")\n",
        "        return mean_loss, mean_metric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now implement the outer loop that trains the model over several epochs.\n",
        "\n",
        "After each epoch, we want to control the model's performance on the validation set.\n",
        "\n",
        "More confisticated training procedures might include model savings, modifying the learning rate or reporting to a dashboard.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_dataloader: data.DataLoader,\n",
        "    validation_dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    metric_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    device: torch.device,\n",
        "    n_epochs: int = 10,\n",
        ") -> tuple[list[float], list[float], list[float]]:\n",
        "    \"\"\"This function should train the model for 10 epochs and return the training and validation losses and metrics\"\"\"\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
        "        # Train the model here\n",
        "\n",
        "        # Evaluate the model here\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_dataloader: data.DataLoader,\n",
        "    validation_dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    metric_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    device: torch.device,\n",
        "    n_epochs: int = 10,\n",
        ") -> tuple[list[float], list[float], list[float]]:\n",
        "    \"\"\"This function should train the model for some epochs and return the training and validation losses\"\"\"\n",
        "    train_losses = []\n",
        "    validation_losses = []\n",
        "    validation_metrics = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
        "        train_loss = train_one_epoch(\n",
        "            model, train_dataloader, loss_fn, optimizer, device\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        validation_loss, validation_metric = evaluate(\n",
        "            model, validation_dataloader, loss_fn, metric_fn, device\n",
        "        )\n",
        "        validation_losses.append(validation_loss)\n",
        "        validation_metrics.append(validation_metric)\n",
        "\n",
        "    return train_losses, validation_losses, validation_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "    DocumentImageDataset(train_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        ")\n",
        "validation_loader = data.DataLoader(\n",
        "    DocumentImageDataset(validation_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "device = torch.device(\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "selected_model = mlp_model\n",
        "\n",
        "optimizer = torch.optim.Adam(selected_model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "metric_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "n_epochs = 5\n",
        "\n",
        "hist = train(\n",
        "    selected_model,\n",
        "    train_loader,\n",
        "    validation_loader,\n",
        "    loss_fn,\n",
        "    metric_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    n_epochs=n_epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the losses and the accuracies on 2 different subplots to observe how the training went.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# First subplot\n",
        "plt.subplot(1, 2, 1)\n",
        "# Subplot code here\n",
        "\n",
        "# Second subplot\n",
        "plt.subplot(1, 2, 2)\n",
        "# Subplot code here\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.plot(hist[0], label=\"Training loss\")\n",
        "plt.plot(hist[1], label=\"Validation loss\")\n",
        "plt.legend()\n",
        "plt.yscale(\"log\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(hist[2], label=\"Validation accuracy\")\n",
        "plt.legend()\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.ylim(0, 1)\n",
        "plt.xlabel(\"Epoch\")\n",
        "\n",
        "# figure title\n",
        "plt.suptitle(\"Training history\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nfLk6pw2M08a"
      },
      "source": [
        "### Evaluation on the test set\n",
        "\n",
        "Now evaluate the model on the remaining test set and store its accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "wq93ddH0NDpS"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "test_loader = data.DataLoader(\n",
        "    DocumentImageDataset(test_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "test_loss, test_metric = evaluate(\n",
        "    mlp_model, test_loader, loss_fn, metric_fn, device, dataset_name=\"test\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x_gKCPtWVg3M"
      },
      "source": [
        "Are these values different from their training counterparts ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LnMkYilGngXs"
      },
      "source": [
        "## Convolutional Neural Networks (CNN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kaz6G-cbotNS"
      },
      "source": [
        "### Training from scratch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gNifb_-ImoL1"
      },
      "source": [
        "Create and compile a model alterning convolution and max pooling layers. You can add some fully connected layers between the last locally connected layer and the output layer. Start with a shallow network (4 or 5 convolution layers) and progressively move to deeper architectures:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5VSo8vjiGOUc"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "conv_model = nn.Sequential(\n",
        "    nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Flatten(start_dim=1),\n",
        "    nn.Linear(128 * 128 * 8, NUM_CLASSES),\n",
        ")\n",
        "\n",
        "print(f\"Your model uses {count_parameters(mlp_model):_} trainable parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the CNN model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "R-fLDDzLmjMH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "    DocumentImageDataset(train_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        ")\n",
        "validation_loader = data.DataLoader(\n",
        "    DocumentImageDataset(validation_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "device = torch.device(\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "selected_model = conv_model\n",
        "\n",
        "optimizer = torch.optim.Adam(selected_model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "metric_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "n_epochs = 5\n",
        "\n",
        "hist = train(\n",
        "    selected_model,\n",
        "    train_loader,\n",
        "    validation_loader,\n",
        "    loss_fn,\n",
        "    metric_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    n_epochs=n_epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation on the test set\n",
        "\n",
        "How does it compare with the MLP model?\n",
        "What is the best accuracy you can get?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "test_loader = data.DataLoader(\n",
        "    DocumentImageDataset(test_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "test_loss, test_metric = evaluate(\n",
        "    conv_model, test_loader, loss_fn, metric_fn, device, dataset_name=\"test\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using a pre-trained model\n",
        "\n",
        "Download a pre-trained model from the [pytoch hub](https://pytorch.org/vision/stable/models.html#using-the-pre-trained-models) for vision model.\n",
        "\n",
        "Eg. Resnet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "from torchvision.models import list_models\n",
        "\n",
        "print(list_models())\n",
        "\n",
        "resnet = torch.hub.load(\"pytorch/vision\", \"efficientnet_b1\", pretrained=True)\n",
        "resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, the loaded does not make predictions for our problem.\n",
        "\n",
        "We need to slightly modify its output to fit our requirements.\n",
        "\n",
        "Models trained on ImageNet expect color images with 3 channels for color instead of 1.\n",
        "We either need to modify th first convolution layer of the model to accomodate for that.\n",
        "Or, another solution could be to repeat our input image 3 times along the channel dimension. That could be done in a new `collat_fn`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet.features[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "# For resnet18\n",
        "# ## convert input to grayscale\n",
        "# resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "# resnet.conv1.weight = nn.Parameter(resnet.conv1.weight.sum(dim=1, keepdim=True) / 3)\n",
        "\n",
        "# ## new classification head\n",
        "# resnet.fc = nn.Linear(resnet.fc.in_features, NUM_CLASSES)\n",
        "\n",
        "\n",
        "# For efficientnet_b1\n",
        "## convert input to grayscale\n",
        "old_weights = nn.Parameter(resnet.features[0][0].weight.sum(dim=1, keepdim=True) / 3)\n",
        "resnet.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "resnet.features[0][0].weight = old_weights\n",
        "\n",
        "resnet.classifier[1] = nn.Linear(resnet.classifier[1].in_features, NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We advice freezing all parameters except those from the last layer of convolution and the new classification head.\n",
        "\n",
        "I will reduce the memory requirements to train the model and ensure the features the pre-trained model was trained to extract are not modified by the finetunig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "for param in resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in resnet.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in resnet.fc.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "    DocumentImageDataset(train_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        ")\n",
        "validation_loader = data.DataLoader(\n",
        "    DocumentImageDataset(validation_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "device = torch.device(\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "selected_model = resnet\n",
        "\n",
        "optimizer = torch.optim.Adam(selected_model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "metric_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "n_epochs = 5\n",
        "\n",
        "hist = train(\n",
        "    selected_model,\n",
        "    train_loader,\n",
        "    validation_loader,\n",
        "    loss_fn,\n",
        "    metric_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    n_epochs=n_epochs,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "j5UeZiGlvNoH",
        "n52k6VoU1brz",
        "nkd4MHFQv0jS",
        "IRPpWCXA05ka",
        "ic-CaNISucO-",
        "XgN4fpA0uO8n",
        "_1Eq6TC2wicn",
        "OdGw-l6TEUiP",
        "nfLk6pw2M08a",
        "XXHM8y9kRW5V"
      ],
      "include_colab_link": true,
      "name": "skeleton.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
