{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clemsage/NeuralDocumentClassification/blob/master/skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j5UeZiGlvNoH"
      },
      "source": [
        "# Setting up the computing environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n52k6VoU1brz"
      },
      "source": [
        "## Install and import PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zZVpUWUluZbV"
      },
      "source": [
        "Select \"GPU\" in the Accelerator drop-down on Notebook Settings through the Edit menu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "8XYfp8LNcRD3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.1+cu121\n"
          ]
        }
      ],
      "source": [
        "# %pip install torch torchvision numpy matplotlib Pillow datasets\n",
        "import torch\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nkd4MHFQv0jS"
      },
      "source": [
        "## Confirm PyTorch can see the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nbotnVwUpWBa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IRPpWCXA05ka"
      },
      "source": [
        "## Additional information about hardware"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k8XAtu4u1mXZ"
      },
      "source": [
        "For CPU information and RAM, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4Mr3-8s-1jPB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 2\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 1\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 2\n",
            "initial apicid\t: 2\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 3\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 1\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 3\n",
            "initial apicid\t: 3\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 4\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 2\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 4\n",
            "initial apicid\t: 4\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 5\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 2\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 5\n",
            "initial apicid\t: 5\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 6\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 3\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 6\n",
            "initial apicid\t: 6\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 7\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 3\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 7\n",
            "initial apicid\t: 7\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 8\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 4\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 8\n",
            "initial apicid\t: 8\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 9\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 4\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 9\n",
            "initial apicid\t: 9\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 10\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 5\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 10\n",
            "initial apicid\t: 10\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 11\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 5\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 11\n",
            "initial apicid\t: 11\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 12\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 6\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 12\n",
            "initial apicid\t: 12\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 13\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 6\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 13\n",
            "initial apicid\t: 13\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 14\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 7\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 14\n",
            "initial apicid\t: 14\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 15\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 7\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 15\n",
            "initial apicid\t: 15\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 16\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 8\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 16\n",
            "initial apicid\t: 16\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 17\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 8\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 17\n",
            "initial apicid\t: 17\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 18\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 9\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 18\n",
            "initial apicid\t: 18\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 19\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 154\n",
            "model name\t: 12th Gen Intel(R) Core(TM) i7-12800H\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2803.212\n",
            "cache size\t: 24576 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 20\n",
            "core id\t\t: 9\n",
            "cpu cores\t: 10\n",
            "apicid\t\t: 19\n",
            "initial apicid\t: 19\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 21\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm md_clear flush_l1d arch_capabilities\n",
            "bugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs retbleed eibrs_pbrsb\n",
            "bogomips\t: 5606.42\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MemTotal:       32708976 kB\n",
            "MemFree:        28445604 kB\n",
            "MemAvailable:   28878440 kB\n",
            "Buffers:           21196 kB\n",
            "Cached:           700264 kB\n",
            "SwapCached:         7356 kB\n",
            "Active:           257032 kB\n",
            "Inactive:        3397928 kB\n",
            "Active(anon):       3316 kB\n",
            "Inactive(anon):  2931968 kB\n",
            "Active(file):     253716 kB\n",
            "Inactive(file):   465960 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:       8388608 kB\n",
            "SwapFree:        8278428 kB\n",
            "Dirty:               148 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:       2830996 kB\n",
            "Mapped:           418724 kB\n",
            "Shmem:              1808 kB\n",
            "KReclaimable:     145644 kB\n",
            "Slab:             223188 kB\n",
            "SReclaimable:     145644 kB\n",
            "SUnreclaim:        77544 kB\n",
            "KernelStack:       12080 kB\n",
            "PageTables:        46532 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:    24743096 kB\n",
            "Committed_AS:    3930684 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:       34292 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             6912 kB\n",
            "AnonHugePages:    833536 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "FileHugePages:         0 kB\n",
            "FilePmdMapped:         0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:       19456 kB\n",
            "DirectMap2M:     5060608 kB\n",
            "DirectMap1G:    36700160 kB\n"
          ]
        }
      ],
      "source": [
        "!cat /proc/cpuinfo\n",
        "!cat /proc/meminfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ic-CaNISucO-"
      },
      "source": [
        "## Other useful package imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gevJulhruagf"
      },
      "outputs": [],
      "source": [
        "import importlib\n",
        "import operator\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from functools import reduce\n",
        "from os import path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0kDf1Kmntpwo"
      },
      "source": [
        "# Working on the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pQe3wu4U5kOt"
      },
      "source": [
        "The dataset is a subset of the [RVL-CDIP dataset](https://www.cs.cmu.edu/~aharley/rvl-cdip/). See [Harley et al.](http://scs.ryerson.ca/~aharley/icdar15/harley_convnet_icdar15.pdf) and [Asim et al.](https://www.dfki.de/fileadmin/user_upload/import/10637_Asim_Document_Image_Classification.pdf) papers for recent works on this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jH2SxwR_1Rpu"
      },
      "source": [
        "## Information about the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lxvfM7Wn_YUQ"
      },
      "source": [
        "This project only considers the following 5 classes among the 16 classes of the original dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "lGLInZca1Pbg"
      },
      "outputs": [],
      "source": [
        "class_names = [\"email\", \"form\", \"handwritten\", \"invoice\", \"advertisement\"]\n",
        "NUM_CLASSES = len(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XgN4fpA0uO8n"
      },
      "source": [
        "## Import the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AVnJr5tzswrq"
      },
      "source": [
        "First, clone or pull the GitHub repository of the project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ST3fUpSmqncY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'NeuralDocumentClassification'...\n",
            "remote: Enumerating objects: 206, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 206 (delta 10), reused 24 (delta 8), pack-reused 177 (from 1)\u001b[K\n",
            "Receiving objects: 100% (206/206), 456.24 KiB | 4.96 MiB/s, done.\n",
            "Resolving deltas: 100% (101/101), done.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(\"NeuralDocumentClassification\"):\n",
        "    !git clone https://github.com/thibaultdouzon/NeuralDocumentClassification.git\n",
        "else:\n",
        "    !git -C NeuralDocumentClassification pull\n",
        "sys.path.append(\"NeuralDocumentClassification\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9zWbX_GYypxH"
      },
      "source": [
        "Download the train, test and validation dataset assignments from this [Google Drive](https://drive.google.com/drive/folders/1Pkd6sUkDGBUymWKK93abZx1MQiWmzFgP):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pQJ8Kqy3sv_v"
      },
      "outputs": [],
      "source": [
        "from src import download_dataset\n",
        "\n",
        "importlib.reload(download_dataset)\n",
        "dataset_path = \"dataset\"\n",
        "\n",
        "download_dataset.download_and_extract(\"all\", dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VYoAg4lOau3h"
      },
      "source": [
        "Each dataset file is a binary dump that can be loaded with the [Pickle](https://docs.python.org/3.11/library/pickle.html) module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "a4G-8jVJauWC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_dataset contains 5000 documents\n",
            "test_dataset contains 1000 documents\n",
            "validation_dataset contains 500 documents\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "dict_keys(['id', 'image', 'label', 'words', 'boxes'])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(path.join(dataset_path, \"train.pkl\"), \"rb\") as f:\n",
        "    train_dataset = pickle.load(f)\n",
        "\n",
        "with open(path.join(dataset_path, \"test.pkl\"), \"rb\") as f:\n",
        "    test_dataset = pickle.load(f)\n",
        "\n",
        "with open(path.join(dataset_path, \"validation.pkl\"), \"rb\") as f:\n",
        "    validation_dataset = pickle.load(f)\n",
        "\n",
        "\n",
        "for split_name, split_dataset in zip(\n",
        "    [\"train\", \"test\", \"validation\"], [train_dataset, test_dataset, validation_dataset]\n",
        "):\n",
        "    print(f\"{split_name}_dataset contains {len(split_dataset)} documents\")\n",
        "train_dataset[0].keys()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each `dataset` object is a `list` containing multiple document information. A document is a `dict` with the following structure:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"id\": \"Unique document identifier\",\n",
        "    \"image\": \"A PIL.Image object containing the document's image\",\n",
        "    \"label\": \"A number between in [0 .. 4] representing the class of the document\",\n",
        "    \"words\": \"A list of words extracted from the image with an OCR\",\n",
        "    \"boxes\": \"A list of tuples of numbers providing the position of each word in the document\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "93EnbGGRid77"
      },
      "source": [
        "Print 5 image from the training dataset using [matplotlib](https://matplotlib.org/stable/tutorials/images.html)'s `plt` module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Lw4UmzI2_FV_"
      },
      "outputs": [],
      "source": [
        "### Insert your code here ###\n",
        "# See the expected solution by clicking on the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "xlg7AAuoiAPU"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "for document in train_dataset[:5]:\n",
        "    print(class_names[document[\"label\"]])\n",
        "    plt.imshow(document[\"image\"].convert(\"RGB\"))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try to answer the following questions:\n",
        "\n",
        "What is the shape of the images?\n",
        "How are the different classes distributed?\n",
        "Using subplots, show an image of each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pTo64W_682WR"
      },
      "source": [
        "# Creating Pytorch datasets and dataloaders for Computer Vision task\n",
        "\n",
        "The first goal of this section is to create `torch.utils.data.Dataset` for the classification task using only the image of the document.\n",
        "\n",
        "We will define a class inheriting [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) called `DocumentImageDataset`.\n",
        "\n",
        "It should be able to create an instance of `DocumentImageDataset` using our previously loaded datasets.\n",
        "For simplification, all images should be formatted to black and white (remove the color channel), and resized to a fixed (512, 512) size. Use [`torchvision.transforms.v2.functional`](https://pytorch.org/vision/main/transforms.html#v2-api-reference-recommended) module to convert a `PIL.Image` to a `torch.Tensor` and perform the simplifications.\n",
        "\n",
        "Upon iteration, it should return an `ImageSample` object defined as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms.v2.functional as F\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ImageSample:\n",
        "    image: torch.Tensor  # shape: (H, W)\n",
        "    label: int  # 0 â‰¤ label < NUM_CLASSES\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"Some assertions to check the validity of the data\"\n",
        "        assert self.image.shape == (\n",
        "            512,\n",
        "            512,\n",
        "        ), f\"Expected shape (512, 512), got {self.image.shape}\"\n",
        "        assert torch.all(self.image <= 1.0) and torch.all(\n",
        "            self.image >= 0.0\n",
        "        ), \"Expected each pixel of image in range [0.0, 1.0]\"\n",
        "        assert self.label in range(\n",
        "            NUM_CLASSES\n",
        "        ), f\"Expected label in range [0 .. {NUM_CLASSES-1}], got {self.label}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "e7QYsgFr88jO"
      },
      "outputs": [],
      "source": [
        "# Fill the methods of the class DocumentImageDataset\n",
        "\n",
        "\n",
        "class DocumentImageDataset(data.Dataset):\n",
        "    def __init__(self, dataset: list[dict]):\n",
        "        self.dataset = dataset\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"This method returns the length of the dataset\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __getitem__(self, idx: int) -> ImageSample:\n",
        "        \"\"\"This method returns the idx-th sample of the dataset\n",
        "        If idx is out of bounds, it should raise an IndexError\"\"\"\n",
        "\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "class DocumentImageDataset(data.Dataset):\n",
        "    def __init__(self, dataset: list[dict]):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"This method returns the length of the dataset\"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> ImageSample:\n",
        "        \"\"\"This method returns the idx-th sample of the dataset\n",
        "        If idx is out of bounds, it should raise an IndexError\"\"\"\n",
        "\n",
        "        return ImageSample(\n",
        "            image=F.to_dtype(\n",
        "                F.to_image(F.resize(self.dataset[idx][\"image\"], size=[512, 512])),\n",
        "                dtype=torch.float32,\n",
        "                scale=True,\n",
        "            ),\n",
        "            label=self.dataset[idx][\"label\"],\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your implementation is correct, you should be able to create an instance of `DocumentImageDataset` and get its 0th element without error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ImageSample(image=tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        ...,\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.]]), label=3)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_dataset = DocumentImageDataset(validation_dataset)\n",
        "image_dataset[0]  # no error here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The final goal of this section is to implement a [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) that wraps the `DocumentImageDataset` and handles useful tasks like shuffling and batching.\n",
        "\n",
        "No need to create a new class, we simply need to implement the `collate_fn` that takes a list of `ImageSample` and should return an `ImageBatch`.\n",
        "\n",
        "\n",
        "hint: Use `torch.tensor` and `torch.stack` to respectively convert a python list to a `torch.Tensor` and stack multiple tensors together into a new one along a new dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ImageBatch:\n",
        "    images: torch.Tensor\n",
        "    labels: torch.Tensor\n",
        "\n",
        "    def __post_init__(self):\n",
        "        assert self.images.shape[0] == self.labels.shape[0]\n",
        "        assert self.images.shape[1:] == (512, 512)\n",
        "        assert len(self.images.shape) == 3\n",
        "        assert len(self.labels.shape) == 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch: list[ImageSample]) -> ImageBatch:\n",
        "    \"\"\"This function should return a batch of samples as an ImageBatch object\"\"\"\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "def collate_fn(batch: list[ImageSample]) -> ImageBatch:\n",
        "    \"\"\"This function should return a batch of samples as an ImageBatch object\"\"\"\n",
        "    return ImageBatch(\n",
        "        images=torch.stack(\n",
        "            [sample.image for sample in batch], dim=0\n",
        "        ),  # shape: (B, H, W)\n",
        "        labels=torch.tensor([sample.label for sample in batch]),  # shape: (B,)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your implementation is correct, you should be able to create a dataloader with a batch size and retrieve the first batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ImageBatch(images=tensor([[[0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         ...,\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
              "\n",
              "        [[0.0000, 0.0000, 0.0196,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [0.0000, 0.0000, 0.1333,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [0.0000, 0.0000, 0.1882,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         ...,\n",
              "         [0.0000, 0.0118, 0.3529,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [0.0000, 0.0235, 0.4431,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [0.0000, 0.0431, 0.5765,  ..., 1.0000, 1.0000, 1.0000]],\n",
              "\n",
              "        [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         ...,\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
              "\n",
              "        [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         ...,\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
              "\n",
              "        [[0.9725, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [0.8588, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [0.8118, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         ...,\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]]), labels=tensor([1, 4, 0, 0, 1]))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataloader = data.DataLoader(\n",
        "    image_dataset, batch_size=5, collate_fn=collate_fn, shuffle=True, drop_last=True\n",
        ")\n",
        "next(iter(dataloader))  # no error here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9oQZqkwA5mRU"
      },
      "source": [
        "# Visual classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JcRvGBPJCPAI"
      },
      "source": [
        "## Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hmD69yYP8Z7X"
      },
      "source": [
        "### Set up the layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ooKloCG47hA4"
      },
      "source": [
        "Build a neural network composed of one fully connected (aka dense, or `Linear` in torch) hidden layer with 128 [ReLu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) units.\n",
        "\n",
        "Each image must be flattened to a single (512 Ã— 512) dimension before being fed to the linear layer.\n",
        "\n",
        "Use `torch.nn` (nn stands for Neural Network) module for all those operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YCIzEefKKKNK"
      },
      "outputs": [],
      "source": [
        "mlp_model = nn.Sequential(\n",
        "    # Fill the layers of the model\n",
        "    # It should take an input of shape (B, 512, 512)\n",
        "    # and output a tensor of shape (B, NUM_CLASSES)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "HYxVCSLQ5tmk"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "mlp_model = nn.Sequential(\n",
        "    nn.Flatten(start_dim=1),  # Do not flatten the batch dimension\n",
        "    nn.Linear(512 * 512, 128),  # d_input = n_pixels in an image = 512 Ã— 512\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, NUM_CLASSES),  # d_output = NUM_CLASSES\n",
        ")\n",
        "\n",
        "conv_model = nn.Sequential(\n",
        "    nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Flatten(start_dim=1),\n",
        "    nn.Linear(128 * 128, NUM_CLASSES),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your model uses 33_555_205 trainable parameters\n",
            "Your model uses 86_725 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    f\"Your model uses {sum([reduce(operator.mul, p.shape, 1) for p in mlp_model.parameters()]):_} trainable parameters\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Your model uses {sum([reduce(operator.mul, p.shape, 1) for p in conv_model.parameters()]):_} trainable parameters\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-GFuLNGh9U5w"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pytorch does not provide a ready to use training loop function like Tensorflow does.\n",
        "We will implement it ourselves.\n",
        "\n",
        "We must first implement the training over a full iteration over the dataloader.\n",
        "It will take the model, the dataloader, a loss function, an optimizer and a device to run on.\n",
        "\n",
        "hint: help yourselves with the torch [documentation](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vO1F5yxIMgY-"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    device: torch.device,\n",
        ") -> float:\n",
        "    \"\"\"This function should train the model for one epoch and return the average loss\"\"\"\n",
        "\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    device: torch.device,\n",
        ") -> float:\n",
        "    \"\"\"This function should train the model for one epoch and return the average loss\"\"\"\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    with tqdm.tqdm(desc=\"Training\", total=len(dataloader)) as pbar:\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            images, labels = batch.images.to(device), batch.labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "            outputs = model(images)  # Compute model's predictions\n",
        "            loss = loss_fn(outputs, labels)  # Compute the loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            pbar.set_postfix(loss=epoch_loss / (i + 1))\n",
        "            pbar.update(1)\n",
        "    mean_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Training loss: {mean_loss:.4f}\")\n",
        "    return mean_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need to implement an evaluation method that evaluates the model's performance on a test or validation set.\n",
        "\n",
        "It might compute the average loss and performance metric that we will use to compare models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    metric_fn: nn.Module,\n",
        "    device: torch.device,\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"This function should evaluate the model on the dataset and return the average loss and metric\"\"\"\n",
        "\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    metric_fn: nn.Module,\n",
        "    device: torch.device,\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"This function should evaluate the model on the dataset and return the average loss and metric\"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    epoch_metric = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(dataloader, desc=\"Evaluation\"):\n",
        "            images, labels = batch.images.to(device), batch.labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            metric = metric_fn(outputs.argmax(dim=-1), labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_metric += metric.item()\n",
        "\n",
        "        mean_loss = epoch_loss / len(dataloader)\n",
        "        mean_metric = epoch_metric / len(dataloader)\n",
        "        print(f\"Evaluation loss: {mean_loss:.4f}\")\n",
        "        print(f\"Evaluation metric: {mean_metric:.4f}\")\n",
        "        return mean_loss, mean_metric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now implement the outer loop that trains the model over several epochs.\n",
        "\n",
        "After each epoch, we want to control the model's performance on the validation set.\n",
        "\n",
        "More confisticated training procedures might include model savings, modifying the learning rate or reporting to a dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_dataloader: data.DataLoader,\n",
        "    validation_dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    metric_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    device: torch.device,\n",
        "    n_epochs: int = 10,\n",
        ") -> tuple[list[float], list[float], list[float]]:\n",
        "    \"\"\"This function should train the model for 10 epochs and return the training and validation losses and metrics\"\"\"\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
        "        # Train the model here\n",
        "\n",
        "        # Evaluate the model here\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_dataloader: data.DataLoader,\n",
        "    validation_dataloader: data.DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    metric_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,  # type: ignore\n",
        "    device: torch.device,\n",
        "    n_epochs: int = 10,\n",
        ") -> tuple[list[float], list[float], list[float]]:\n",
        "    \"\"\"This function should train the model for some epochs and return the training and validation losses\"\"\"\n",
        "    train_losses = []\n",
        "    validation_losses = []\n",
        "    validation_metrics = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
        "        train_loss = train_one_epoch(\n",
        "            model, train_dataloader, loss_fn, optimizer, device\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        validation_loss, validation_metric = evaluate(\n",
        "            model, validation_dataloader, loss_fn, metric_fn, device\n",
        "        )\n",
        "        validation_losses.append(validation_loss)\n",
        "        validation_metrics.append(validation_metric)\n",
        "\n",
        "    return train_losses, validation_losses, validation_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/313 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [16, 1, 3, 3], expected input[1, 16, 512, 512] to have 1 channels, but got 16 channels instead",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[40], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m metric_fn \u001b[38;5;241m=\u001b[39m torchmetrics\u001b[38;5;241m.\u001b[39mAccuracy(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39mNUM_CLASSES)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m---> 23\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[35], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, validation_dataloader, loss_fn, metric_fn, optimizer, device, n_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     26\u001b[0m     validation_loss, validation_metric \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m     27\u001b[0m         model, validation_dataloader, loss_fn, metric_fn, device\n\u001b[1;32m     28\u001b[0m     )\n",
            "Cell \u001b[0;32mIn[33], line 19\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mimages\u001b[38;5;241m.\u001b[39mto(device), batch\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Reset gradients\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute model's predictions\u001b[39;00m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)  \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/code/NeuralDocumentClassification/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/code/NeuralDocumentClassification/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/code/NeuralDocumentClassification/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/code/NeuralDocumentClassification/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/code/NeuralDocumentClassification/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/code/NeuralDocumentClassification/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/code/NeuralDocumentClassification/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 1, 3, 3], expected input[1, 16, 512, 512] to have 1 channels, but got 16 channels instead"
          ]
        }
      ],
      "source": [
        "import torchmetrics\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "    DocumentImageDataset(train_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        ")\n",
        "validation_loader = data.DataLoader(\n",
        "    DocumentImageDataset(validation_dataset),\n",
        "    batch_size=16,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "metric_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "n_epochs = 5\n",
        "\n",
        "hist = train(\n",
        "    conv_model,\n",
        "    train_loader,\n",
        "    validation_loader,\n",
        "    loss_fn,\n",
        "    metric_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    n_epochs=n_epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nfLk6pw2M08a"
      },
      "source": [
        "### Evaluation on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "wq93ddH0NDpS"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "model.evaluate(labeled_test_ds, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x_gKCPtWVg3M"
      },
      "source": [
        "Are these values different from their training counterparts ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XXHM8y9kRW5V"
      },
      "source": [
        "### Prediction on the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5xHWavjURp-t"
      },
      "source": [
        "Implement a function that gathers the model predictions and the ground truth labels for a random batch of a given dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hq68PQDJsbXh"
      },
      "outputs": [],
      "source": [
        "def predict_random_batch(model, dataset):\n",
        "    \"\"\"\n",
        "    Sample a random batch of the dataset and return the images of this batch\n",
        "    as well as its labels and the predicted classes of a given model\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : tf.keras.Model\n",
        "    dataset: tf.data.Dataset\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    images: np.ndarray or EagerTensor\n",
        "    labels: list of str\n",
        "    predicted_classes: list of str\n",
        "    \"\"\"\n",
        "    images, labels = next(iter(dataset))\n",
        "\n",
        "    # get label names for the sampled batch\n",
        "\n",
        "    # make predictions\n",
        "    predicted_classes = None\n",
        "\n",
        "    return images, labels, predicted_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "xX0MIEONRkp7"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def predict_random_batch(model, dataset):\n",
        "    \"\"\"\n",
        "    Sample a random batch of the dataset and return the images of this batch\n",
        "    as well as its labels and the predicted classes of a given model\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : tf.keras.Model\n",
        "    dataset: tf.data.Dataset\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    images: np.ndarray or EagerTensor\n",
        "    labels: list of str\n",
        "    predicted_classes: list of str\n",
        "    \"\"\"\n",
        "    images, labels = next(iter(dataset))\n",
        "\n",
        "    # get label names for the sampled batch\n",
        "    labels = [class_names[i] for i in labels]\n",
        "\n",
        "    # make predictions\n",
        "    predictions = model.predict(images)\n",
        "    predicted_classes_idx = np.argmax(predictions, axis=1)\n",
        "    predicted_classes = [class_names[i] for i in predicted_classes_idx]\n",
        "\n",
        "    return images, labels, predicted_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uyZFGQLLVFeC"
      },
      "source": [
        "Plot the first 9 images of this batch, give their labels and predicted classes in the legend:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wrgwVDo4VFG7"
      },
      "outputs": [],
      "source": [
        "def plot_images_predictions_and_labels(images, labels, predicted_classes):\n",
        "    plt.figure(figsize=(30, 40))\n",
        "\n",
        "    for im_idx in range(9):\n",
        "        plt.subplot(3, 3, im_idx + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(np.squeeze(images[im_idx]), cmap=\"gray\")\n",
        "        plt.xlabel(\"label: %s\\npred: %s\" % (labels[im_idx], predicted_classes[im_idx]))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "result = predict_random_batch(model, labeled_test_ds)\n",
        "plot_images_predictions_and_labels(*result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "monzP4ZVVkGr"
      },
      "source": [
        "### Under the Hood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "61YBF5sLfsrZ"
      },
      "source": [
        "Implement an ReLu dense layer by creating its weights and biases and giving the transformation from inputs to outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8_-Q8GwoChTv"
      },
      "outputs": [],
      "source": [
        "# https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_layer_class\n",
        "class MyDenseLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, input_dim):\n",
        "        super(MyDenseLayer, self).__init__()\n",
        "        self.w = self.add_weight(\n",
        "            shape=None,  ## Insert the shape of the weight matrix here\n",
        "            initializer=\"glorot_uniform\",  # Default initializer for weights of a tf.keras.layers.Dense layer\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        self.b = self.add_weight(\n",
        "            shape=None,  ## Insert the shape of the bias vector here\n",
        "            initializer=\"zeros\",  # Default initializer for biases of a tf.keras.layers.Dense layer\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = None\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "5HWRDQkPfBam"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_layer_class\n",
        "class MyDenseLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, input_dim):\n",
        "        super(MyDenseLayer, self).__init__()\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_dim, units),\n",
        "            initializer=\"glorot_uniform\",  # Default initializer for weights of a tf.keras.layers.Dense layer\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        self.b = self.add_weight(\n",
        "            shape=(units,),\n",
        "            initializer=\"zeros\",  # Default initializer for biases of a tf.keras.layers.Dense layer\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.keras.activations.relu(tf.matmul(inputs, self.w) + self.b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VyPOuiP_5ZGv"
      },
      "source": [
        "Using your custom hidden layer, set up again the layers of the model defined previously:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8fK9p35mD9eI"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Flatten(input_shape=(IMG_HEIGHT, IMG_WIDTH, 1)),\n",
        "        ## Insert your custom hidden layer here\n",
        "        tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "CBd68qImh1zH"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Flatten(input_shape=(IMG_HEIGHT, IMG_WIDTH, 1)),\n",
        "        MyDenseLayer(128, IMG_HEIGHT * IMG_WIDTH),\n",
        "        tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eVgtyKeNhXJR"
      },
      "source": [
        "Lower-level implementation of the model compile step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-4IEM4Rhg4ji"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images)\n",
        "        loss = loss_object(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VxMO3ZB7ic0-"
      },
      "source": [
        "Lower-level implementation of the model fit step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Mo77GEbdifjm"
      },
      "outputs": [],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    for images, labels in labeled_train_ds:\n",
        "        train_step(images, labels)\n",
        "        template = \"Epoch {}, Loss: {}, Accuracy: {}\"\n",
        "        print(\n",
        "            template.format(\n",
        "                epoch + 1, train_loss.result(), train_accuracy.result() * 100\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LnMkYilGngXs"
      },
      "source": [
        "## Convolutional Neural Networks (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kaz6G-cbotNS"
      },
      "source": [
        "### Training from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gNifb_-ImoL1"
      },
      "source": [
        "Create and compile a model alterning convolution and max pooling layers. You can add some fully connected layers between the last locally connected layer and the output layer. Start with a shallow network (4 or 5 convolution layers) and progressively move to deeper architectures: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5VSo8vjiGOUc"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
        "\n",
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        # Alterning Conv2D and MaxPooling2D layers\n",
        "        # Some dense hidden layer(s)\n",
        "        Dense(NUM_CLASSES, activation=\"softmax\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "R-fLDDzLmjMH"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "shallow_model = tf.keras.Sequential(\n",
        "    [\n",
        "        Conv2D(\n",
        "            16,\n",
        "            3,\n",
        "            padding=\"same\",\n",
        "            activation=\"relu\",\n",
        "            input_shape=(IMG_HEIGHT, IMG_WIDTH, 1),\n",
        "        ),\n",
        "        MaxPooling2D(4),\n",
        "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "        MaxPooling2D(4),\n",
        "        Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
        "        MaxPooling2D(4),\n",
        "        Conv2D(128, 3, padding=\"same\", activation=\"relu\"),\n",
        "        MaxPooling2D(4),\n",
        "        Flatten(),\n",
        "        Dense(128, activation=\"relu\"),\n",
        "        Dense(NUM_CLASSES, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "deep_model = tf.keras.Sequential(\n",
        "    [\n",
        "        Conv2D(\n",
        "            16,\n",
        "            3,\n",
        "            padding=\"same\",\n",
        "            activation=\"relu\",\n",
        "            input_shape=(IMG_HEIGHT, IMG_WIDTH, 1),\n",
        "        ),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(128, 3, padding=\"same\", activation=\"relu\"),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(256, 3, padding=\"same\", activation=\"relu\"),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(256, 3, padding=\"same\", activation=\"relu\"),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(256, 3, padding=\"same\", activation=\"relu\"),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(256, 3, padding=\"same\", activation=\"relu\"),\n",
        "        MaxPooling2D(),\n",
        "        Flatten(),\n",
        "        Dense(256, activation=\"relu\"),\n",
        "        Dense(NUM_CLASSES, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model_with_strides = tf.keras.Sequential(\n",
        "    [\n",
        "        Conv2D(\n",
        "            16,\n",
        "            3,\n",
        "            padding=\"same\",\n",
        "            activation=\"relu\",\n",
        "            input_shape=(IMG_HEIGHT, IMG_WIDTH, 1),\n",
        "        ),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(64, 3, padding=\"same\", activation=\"relu\", strides=2),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(128, 3, padding=\"same\", activation=\"relu\", strides=2),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(128, 3, padding=\"same\", activation=\"relu\", strides=2),\n",
        "        MaxPooling2D(),\n",
        "        Flatten(),\n",
        "        Dense(128, activation=\"relu\"),\n",
        "        Dense(NUM_CLASSES, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "model = deep_model\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FVzSjxP0qP4D"
      },
      "source": [
        "Fit the CNN on the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3jWd_WMpqPi8"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "model.fit(labeled_train_ds, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9mGpqBc7rnFv"
      },
      "source": [
        "Evaluate the trained model on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tSmadMoVrmnf"
      },
      "outputs": [],
      "source": [
        "model.evaluate(labeled_test_ds, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xK3wwRUPOBEs"
      },
      "source": [
        "You should reach test accuracy greater than 0.99 !\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7S7XNOyD3pRI"
      },
      "source": [
        "Plot images, predictions and labels for some test documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FZ4le3PQuWYo"
      },
      "outputs": [],
      "source": [
        "plot_images_predictions_and_labels(*predict_random_batch(model, labeled_test_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ArNeT3D1nINR"
      },
      "source": [
        "### Transfer Learning with pre-trained models "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PS-BCUYYRr_I"
      },
      "source": [
        "The objective is to leverage the knowledge learnt by a pre-trained image classifier. See [TensorFlow Hub](https://tfhub.dev/) to browse available state-of-the art models such as [Inception V3](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf) or [MobileNet V2](https://arxiv.org/pdf/1801.04381.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Q7LRLglIQMY"
      },
      "source": [
        "Choose a pre-trained model for extracting high level feature vectors of document images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5F0WHgxTJX0S"
      },
      "outputs": [],
      "source": [
        "extractor_model = \"inception_v3\"\n",
        "if extractor_model == \"inception_v3\":\n",
        "    feature_extraction_url = (\n",
        "        \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
        "    )\n",
        "    IMG_HEIGHT, IMG_WIDTH = None, None  ## Insert expected input image shape here\n",
        "elif extractor_model == \"mobilenet_v2\":\n",
        "    feature_extraction_url = (\n",
        "        \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\"\n",
        "    )\n",
        "    IMG_HEIGHT, IMG_WIDTH = None, None  ## Insert expected input image shape here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "ho18UyThppL6"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "extractor_model = \"inception_v3\"\n",
        "if extractor_model == \"inception_v3\":\n",
        "    feature_extraction_url = (\n",
        "        \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
        "    )\n",
        "    IMG_HEIGHT, IMG_WIDTH = 299, 299\n",
        "elif extractor_model == \"mobilenet_v2\":\n",
        "    feature_extraction_url = (\n",
        "        \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\"\n",
        "    )\n",
        "    IMG_HEIGHT, IMG_WIDTH = 224, 224"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WS2LwuZGPfg_"
      },
      "source": [
        "Reshape images to the format expected by the chosen model, i.e. IMG_HEIGHT x IMG_WIDTH x 3 (RGB) and recreate training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "D51vnWCPPunJ"
      },
      "outputs": [],
      "source": [
        "def decode_img(img):\n",
        "    # convert the compressed string to a uint8 tensor\n",
        "    img = tf.io.decode_png(img, channels=1)\n",
        "    # convert to floats in the [0,1] range\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    # resize the image to the desired size\n",
        "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
        "    # convert to RGB color scale\n",
        "    img = tf.concat([img for _ in range(3)], axis=-1)  # R = G = B\n",
        "    return img\n",
        "\n",
        "\n",
        "def process_path(file_path):\n",
        "    label = labels_idx.lookup(file_path)\n",
        "\n",
        "    img = tf.io.read_file(file_path)\n",
        "    img = decode_img(img)\n",
        "    return img, label\n",
        "\n",
        "\n",
        "labeled_train_ds = list_train_ds.map(\n",
        "    process_path, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        ")\n",
        "labeled_train_ds = labeled_train_ds.cache(\"/tmp/%dx%dx3\" % (IMG_HEIGHT, IMG_WIDTH))\n",
        "labeled_train_ds = labeled_train_ds.shuffle(2048).batch(batch_size)\n",
        "labeled_train_ds = labeled_train_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1WXZOzEvnINO"
      },
      "source": [
        "Construct our own image classifier by retrieving and freezing the hidden layers of the pre-trained model: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JRq-5IVoMQFD"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "aDMImRlEnINF"
      },
      "outputs": [],
      "source": [
        "# @title\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0fx9gcSRaKf"
      },
      "source": [
        "Train this new model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gecTChcfRc-K"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "model.fit(labeled_train_ds, epochs=EPOCHS)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "j5UeZiGlvNoH",
        "n52k6VoU1brz",
        "nkd4MHFQv0jS",
        "IRPpWCXA05ka",
        "ic-CaNISucO-",
        "XgN4fpA0uO8n",
        "_1Eq6TC2wicn",
        "OdGw-l6TEUiP",
        "nfLk6pw2M08a",
        "XXHM8y9kRW5V"
      ],
      "include_colab_link": true,
      "name": "skeleton.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
